{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb26c6f-4974-4da1-a77a-6609e22afe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. ç¯å¢ƒä¸ä¾èµ– ==========\n",
    "import os, json, warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import shap, math\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, recall_score, confusion_matrix, precision_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.utils import check_random_state\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logit, expit\n",
    "from numpy.random import default_rng\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "\n",
    "\n",
    "# ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªéšæœºç§å­\n",
    "RANDOM_STATE = 2025\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def setup_chinese_fonts():\n",
    "    \"\"\"\n",
    "    é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º\n",
    "    \"\"\"\n",
    "    # æ–¹æ¡ˆ1ï¼šä½¿ç”¨ç³»ç»Ÿä¸­æ–‡å­—ä½“ï¼ˆæ¨èï¼‰\n",
    "    try:\n",
    "        rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']\n",
    "        rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "        print(\"âœ… ä¸­æ–‡å­—ä½“é…ç½®æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ä¸­æ–‡å­—ä½“é…ç½®å¤±è´¥: {e}\")\n",
    "        print(\"   ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ\")\n",
    "\n",
    "# ç«‹å³é…ç½®\n",
    "setup_chinese_fonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0e552-b3ed-4bd9-9dd7-9b5c93ea0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. è¯»å–æ•°æ® ==========\n",
    "DATA_PATH = r\"C:/Users/VVai/Desktop/output/æ¸…æ´—/æ•´åˆæ•°æ®æ•´ç†/output_with_CCI_total.csv\"\n",
    "\n",
    "TARGET = \"death_flag\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"æ•°æ®å½¢çŠ¶:\", df.shape)\n",
    "print(\"åˆ—å(é¢„è§ˆ)å‰30ä¸ª:\", list(df.columns)[:30])\n",
    "assert TARGET in df.columns, f\"æœªæ‰¾åˆ°ç›®æ ‡å˜é‡åˆ— {TARGET}\"\n",
    "\n",
    "# å€™é€‰ç‰¹å¾\n",
    "candidate_cols = [\n",
    "    # åŸºæœ¬ä¿¡æ¯\n",
    "    \"age\", \"gender\",\n",
    "\n",
    "    # ä½“æ ¼ä¿¡æ¯\n",
    "    \"Height\", \"Height (Inches)\", \"Weight\", \"Weight (Lbs)\", \"BMI\", \"BMI (kg/m2)\",\n",
    "\n",
    "    # è¡€å‹\n",
    "    \"Diastolic blood pressure\", \"Systolic blood pressure\",\n",
    "\n",
    "    # ç”²åŠŸç›¸å…³\n",
    "    \"PTFQI\",\n",
    "\n",
    "    # è¡€æ¶²/ç”ŸåŒ–æŒ‡æ ‡\n",
    "    \"RBC(m/uL)\", \"WBC(K/uL)\", \"PLT\", \"Hb(g/dL)\", \"Glucose(mg/dL)\",\n",
    "    \"Creatinine(mg/dL)\", \"BUN(mg/dL)\", \"ALT(IU/L)\", \"AST\", \"PT\",\n",
    "\n",
    "    # ç–¾ç—…/å¹¶å‘ç—‡\n",
    "    \"Hypertension\", \"Diabetes\", \"CKD\", \"Myocardial infarct\",\n",
    "    \"Congestive heart failure\", \"Malignant cancer\", \"COPD\",\n",
    "\n",
    "    # ä½é™¢ä¿¡æ¯ï¼ˆæ ¹æ®ç›®æ ‡å˜é‡è°ƒæ•´ï¼‰\n",
    "    \"CCI Score\",\n",
    "]\n",
    "\n",
    "#ä»ç‰¹å¾ä¸­æ’é™¤ç›®æ ‡å˜é‡\n",
    "feature_cols = [c for c in candidate_cols if c in df.columns and c != TARGET]\n",
    "\n",
    "# 1ï¼šç¡®ä¿ç›®æ ‡å˜é‡ä¸åœ¨ç‰¹å¾ä¸­\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” å®‰å…¨æ£€æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç›®æ ‡å˜é‡: {TARGET}\")\n",
    "print(f\"å€™é€‰ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "\n",
    "if TARGET in feature_cols:\n",
    "    print(\"ä¸¥é‡é”™è¯¯ï¼šç›®æ ‡å˜é‡å‡ºç°åœ¨ç‰¹å¾åˆ—è¡¨ä¸­ï¼\")\n",
    "    print(\"è¿™ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼Œç»“æœå®Œå…¨ä¸å¯ä¿¡ï¼\")\n",
    "    raise ValueError(f\"ç›®æ ‡å˜é‡ {TARGET} ä¸èƒ½ä½œä¸ºç‰¹å¾ï¼\")\n",
    "else:\n",
    "    print(f\"é€šè¿‡ï¼šç›®æ ‡å˜é‡ '{TARGET}' æœªå‡ºç°åœ¨ç‰¹å¾ä¸­\")\n",
    "\n",
    "# 2ï¼šæ‰“å°ç‰¹å¾åˆ—è¡¨ä¾›äººå·¥å®¡æ ¸\n",
    "print(f\"\\nç”¨äºå»ºæ¨¡çš„ç‰¹å¾åˆ—è¡¨ ({len(feature_cols)} ä¸ª):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# æ€§åˆ«æ˜ å°„(å¦‚æ˜¯å­—ç¬¦ä¸²)\n",
    "if \"gender\" in feature_cols and df[\"gender\"].dtype == object:\n",
    "    df[\"gender\"] = df[\"gender\"].map({\"M\":1, \"F\":0}).fillna(0).astype(int)\n",
    "    print(\"\\n æ€§åˆ«å·²æ˜ å°„: M â†’ 1, F â†’ 0\")\n",
    "\n",
    "# æ„å»ºå»ºæ¨¡æ•°æ®é›†\n",
    "df_model = df[feature_cols + [TARGET]].dropna().copy()\n",
    "X_all = df_model[feature_cols]\n",
    "y_all = df_model[TARGET].astype(int)\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ ·æœ¬æ•°é‡: {X_all.shape[0]}\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_all.shape[1]}\")\n",
    "print(f\"ç›®æ ‡å˜é‡: {TARGET}\")\n",
    "print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "print(y_all.value_counts())\n",
    "print(f\"æ­£ç±»æ¯”ä¾‹: {y_all.mean():.2%}\")\n",
    "\n",
    "# ç¡®è®¤ X_all ä¸­æ²¡æœ‰ç›®æ ‡å˜é‡\n",
    "if TARGET in X_all.columns:\n",
    "    raise ValueError(f\" è‡´å‘½é”™è¯¯ï¼šç›®æ ‡å˜é‡ {TARGET} åœ¨ç‰¹å¾çŸ©é˜µä¸­ï¼\")\n",
    "print(f\"\\n æœ€ç»ˆç¡®è®¤ï¼šç‰¹å¾çŸ©é˜µä¸­ä¸å«ç›®æ ‡å˜é‡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åœ¨ç¬¬2æ­¥æœ«å°¾æ·»åŠ \n",
    "print(f\"\\n ç±»åˆ«åˆ†å¸ƒåˆ†æ:\")\n",
    "print(f\"  è´Ÿç±»(å­˜æ´»): {(y_all==0).sum()} ({(y_all==0).mean():.1%})\")\n",
    "print(f\"  æ­£ç±»(æ­»äº¡): {(y_all==1).sum()} ({(y_all==1).mean():.1%})\")\n",
    "print(f\"  ä¸å¹³è¡¡æ¯”: 1:{(y_all==0).sum()/(y_all==1).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬3æ­¥:æ•°æ®é›†åˆ’åˆ† ============================\n",
    "# Step 1: å…ˆåˆ’åˆ†å‡ºæµ‹è¯•é›†(10%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_all  # ä¿è¯ç±»åˆ«æ¯”ä¾‹ä¸€è‡´\n",
    ")\n",
    "\n",
    "# Step 2: å‰©ä¸‹çš„ 90% å†æŒ‰ 8:1 åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=1/9,   # 90% çš„ 1/9 â‰ˆ 10%\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# ä¿å­˜å‰¯æœ¬(ç”¨äºå½’ä¸€åŒ– / ç‰¹å¾é€‰æ‹©)\n",
    "train_y = y_train.copy()\n",
    "val_y = y_val.copy()\n",
    "test_y = y_test.copy()\n",
    "\n",
    "\n",
    "print(\"æ•°æ®é›†åˆ’åˆ†å®Œæ¯•:\")\n",
    "print(f\"è®­ç»ƒé›†:{X_train.shape}, éªŒè¯é›†:{X_val.shape}, æµ‹è¯•é›†:{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬4æ­¥:æ•°æ®é¢„å¤„ç† ============================\n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æœ€ç»ˆå˜é‡åï¼Œé¿å…ä¸­é—´å˜é‡\n",
    "train_X_scaled = X_train.copy()\n",
    "val_X_scaled = X_val.copy()\n",
    "test_X_scaled = X_test.copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¼€å§‹æ•°æ®é¢„å¤„ç†\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è®­ç»ƒé›†: {train_X_scaled.shape}\")\n",
    "print(f\"éªŒè¯é›†: {val_X_scaled.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {test_X_scaled.shape}\")\n",
    "\n",
    "# è¯†åˆ«å˜é‡ç±»å‹\n",
    "binary_cols = []\n",
    "cont_cols = []\n",
    "\n",
    "for col in train_X_scaled.columns:\n",
    "    unique_vals = train_X_scaled[col].dropna().unique()\n",
    "    if set(unique_vals).issubset({0, 1}) or set(unique_vals).issubset({0.0, 1.0}):\n",
    "        binary_cols.append(col)\n",
    "    else:\n",
    "        cont_cols.append(col)\n",
    "\n",
    "print(f\"\\näºŒåˆ†ç±»å˜é‡ ({len(binary_cols)}ä¸ª): {binary_cols}\")\n",
    "print(f\"è¿ç»­å˜é‡ ({len(cont_cols)}ä¸ª): {cont_cols}\")\n",
    "\n",
    "# ç¼ºå¤±å€¼å¡«å……\n",
    "if len(cont_cols) > 0:\n",
    "    imputer_cont = SimpleImputer(strategy='median')\n",
    "    train_X_scaled[cont_cols] = imputer_cont.fit_transform(train_X_scaled[cont_cols])\n",
    "    val_X_scaled[cont_cols] = imputer_cont.transform(val_X_scaled[cont_cols])\n",
    "    test_X_scaled[cont_cols] = imputer_cont.transform(test_X_scaled[cont_cols])\n",
    "    print(f\" è¿ç»­å˜é‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ (ä¸­ä½æ•°)\")\n",
    "\n",
    "if len(binary_cols) > 0:\n",
    "    imputer_binary = SimpleImputer(strategy='most_frequent')\n",
    "    train_X_scaled[binary_cols] = imputer_binary.fit_transform(train_X_scaled[binary_cols])\n",
    "    val_X_scaled[binary_cols] = imputer_binary.transform(val_X_scaled[binary_cols])\n",
    "    test_X_scaled[binary_cols] = imputer_binary.transform(test_X_scaled[binary_cols])\n",
    "    print(f\" äºŒåˆ†ç±»å˜é‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ (ä¼—æ•°)\")\n",
    "\n",
    "# åæ–œå¤„ç†\n",
    "skew_threshold = 2.0\n",
    "highly_skewed = []\n",
    "for col in cont_cols:\n",
    "    skewness = train_X_scaled[col].skew()\n",
    "    if abs(skewness) > skew_threshold and train_X_scaled[col].min() >= 0:\n",
    "        highly_skewed.append(col)\n",
    "\n",
    "if len(highly_skewed) > 0:\n",
    "    print(f\"\\næ£€æµ‹åˆ° {len(highly_skewed)} ä¸ªé«˜åæ–œå˜é‡: {highly_skewed}\")\n",
    "    train_X_scaled[highly_skewed] = np.log1p(train_X_scaled[highly_skewed])\n",
    "    val_X_scaled[highly_skewed] = np.log1p(val_X_scaled[highly_skewed])\n",
    "    test_X_scaled[highly_skewed] = np.log1p(test_X_scaled[highly_skewed])\n",
    "    print(f\" å·²è¿›è¡Œ log1p è½¬æ¢\")\n",
    "\n",
    "# æ ‡å‡†åŒ– (ä»…å¯¹è¿ç»­å˜é‡)\n",
    "scaler = RobustScaler()\n",
    "train_X_scaled[cont_cols] = scaler.fit_transform(train_X_scaled[cont_cols])\n",
    "val_X_scaled[cont_cols] = scaler.transform(val_X_scaled[cont_cols])\n",
    "test_X_scaled[cont_cols] = scaler.transform(test_X_scaled[cont_cols])\n",
    "\n",
    "print(f\"\\n é¢„å¤„ç†å®Œæˆ\")\n",
    "print(f\"   - ç¼ºå¤±å€¼å¡«å……: \")\n",
    "print(f\"   - åæ–œå¤„ç†: {len(highly_skewed)} ä¸ªå˜é‡\")\n",
    "print(f\"   - æ ‡å‡†åŒ–: {len(cont_cols)} ä¸ªè¿ç»­å˜é‡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åˆ›å»ºå˜é‡åˆ«åï¼Œä¾›åç»­å¯è§†åŒ–ä½¿ç”¨\n",
    "train_X_imp = train_X_scaled.copy()\n",
    "val_X_imp = val_X_scaled.copy()\n",
    "test_X_imp = test_X_scaled.copy()\n",
    "\n",
    "print(f\" å·²åˆ›å»ºé¢„å¤„ç†åçš„æ•°æ®å‰¯æœ¬: train_X_imp, val_X_imp, test_X_imp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬5æ­¥:Boruta ç‰¹å¾é€‰æ‹© ============================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"å¼€å§‹ Boruta ç‰¹å¾é€‰æ‹©\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Boruta å‚æ•°è®¾ç½®\n",
    "selector = BorutaPy(\n",
    "    RandomForestClassifier(\n",
    "        n_jobs=-1, \n",
    "        class_weight='balanced', \n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    n_estimators='auto',\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "selector.fit(train_X_scaled.values, train_y.values)\n",
    "\n",
    "# è·å–é€‰ä¸­çš„ç‰¹å¾\n",
    "selected_features = train_X_scaled.columns[selector.support_].tolist()\n",
    "tentative_features = train_X_scaled.columns[selector.support_weak_].tolist()\n",
    "selected_all = selected_features + tentative_features\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\" Boruta ç‰¹å¾é€‰æ‹©å®Œæˆ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ç¡®è®¤é€‰ä¸­çš„ç‰¹å¾ ({len(selected_features)}): {selected_features}\")\n",
    "print(f\"å¾…å®šç‰¹å¾ ({len(tentative_features)}): {tentative_features}\")\n",
    "print(f\"æœ€ç»ˆä½¿ç”¨çš„ç‰¹å¾æ€»æ•° ({len(selected_all)}): {selected_all}\")\n",
    "\n",
    "# ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': train_X_scaled.columns,\n",
    "    'Selected': selector.support_,\n",
    "    'Tentative': selector.support_weak_,\n",
    "    'Ranking': selector.ranking_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "print(\"\\nç‰¹å¾é‡è¦æ€§æ’å:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"åº”ç”¨ç‰¹å¾é€‰æ‹©åˆ°æ‰€æœ‰æ•°æ®é›†\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_X_sel = train_X_scaled[selected_all].copy()\n",
    "val_X_sel = val_X_scaled[selected_all].copy()\n",
    "test_X_sel = test_X_scaled[selected_all].copy()\n",
    "\n",
    "# ç»Ÿä¸€å˜é‡åï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨\n",
    "X_train_for_models = train_X_sel\n",
    "X_val_for_models = val_X_sel\n",
    "X_test_for_models = test_X_sel\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {train_X_sel.shape}\")\n",
    "print(f\"éªŒè¯é›†: {val_X_sel.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {test_X_sel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Boruta å†…éƒ¨æœºåˆ¶å¯è§†åŒ–ï¼ˆæ ‡å‡†Borutaæ–¹å¼ï¼švs. Shadow Maxï¼‰=====================\n",
    "# ---------- å¯é…ç½® ----------\n",
    "N_ITERS = 200\n",
    "SAMPLE_FRAC = 0.8\n",
    "SEED = 42\n",
    "\n",
    "# ----------  ä½¿ç”¨å»ºæ¨¡æ—¶ç›¸åŒçš„ç‰¹å¾é›† ----------\n",
    "X_sel = train_X_scaled[selected_all].copy()\n",
    "y_sel = train_y.copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¨ Boruta å±±è„Šå›¾å¯è§†åŒ–ï¼ˆæ ‡å‡†Borutaæ–¹å¼ï¼švs. Shadow Maxï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¯è§†åŒ–ç‰¹å¾æ•°: {len(selected_all)}\")\n",
    "print(f\"  - ç¡®è®¤ç‰¹å¾: {len(selected_features)}\")\n",
    "print(f\"  - å¾…å®šç‰¹å¾: {len(tentative_features)}\")\n",
    "print(f\"è¿­ä»£æ¬¡æ•°: {N_ITERS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------- RF å‚æ•° ----------\n",
    "if 'selector' in globals():\n",
    "    RF_PARAMS = selector.estimator.get_params()\n",
    "    RF_PARAMS.pop('random_state', None)\n",
    "else:\n",
    "    RF_PARAMS = dict(\n",
    "        n_estimators=300,\n",
    "        n_jobs=-1,\n",
    "        max_depth=5,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "# ---------- ç”Ÿæˆæ ‡å‡†Boruta Z-scoreåˆ†å¸ƒï¼ˆä¸Shadow Maxæ¯”è¾ƒï¼‰ ----------\n",
    "def collect_boruta_z_vs_shadow_max(X_df, y, rf_params, n_iters=200, sample_frac=0.8, seed=42):\n",
    "    rng = default_rng(seed)\n",
    "    cols = list(X_df.columns)\n",
    "    n = len(X_df)\n",
    "    take = max(1, int(n * float(sample_frac)))\n",
    "    rows = []\n",
    "\n",
    "    base_params = dict(rf_params)\n",
    "    base_params.pop(\"random_state\", None)\n",
    "\n",
    "    for i in tqdm(range(n_iters), desc=\"Collecting Boruta Z (vs. Shadow Max)\"):\n",
    "        idx = rng.choice(n, take, replace=True)\n",
    "        Xb = X_df.iloc[idx, :]\n",
    "        yb = y.iloc[idx]\n",
    "\n",
    "        X_shadow = pd.DataFrame(index=Xb.index)\n",
    "        for c in cols:\n",
    "            shuf = Xb[c].to_numpy().copy()\n",
    "            rng.shuffle(shuf)\n",
    "            X_shadow[c + \"_shadow\"] = shuf\n",
    "\n",
    "        X_aug = pd.concat([Xb.reset_index(drop=True), X_shadow.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        rf_local = RandomForestClassifier(**base_params, random_state=int(seed) + i)\n",
    "        rf_local.fit(X_aug, yb.values)\n",
    "\n",
    "        imps = pd.Series(rf_local.feature_importances_, index=X_aug.columns)\n",
    "        imp_real   = imps.loc[cols].values\n",
    "        imp_shadow = imps.loc[[c + \"_shadow\" for c in cols]].values\n",
    "\n",
    "        # ä½¿ç”¨shadowæœ€å¤§å€¼ï¼ˆMZSAï¼‰\n",
    "        sh_max = float(np.max(imp_shadow))\n",
    "        sh_std = float(np.std(imp_shadow))\n",
    "        \n",
    "        if sh_std == 0 or np.isnan(sh_std):\n",
    "            sh_std = 1e-12\n",
    "        \n",
    "        z = (imp_real - sh_max) / sh_std\n",
    "\n",
    "        rows.extend([(f, float(zv), i) for f, zv in zip(cols, z)])\n",
    "\n",
    "    long_df = pd.DataFrame(rows, columns=[\"feature\", \"z\", \"iter\"])\n",
    "    return long_df\n",
    "\n",
    "long_borutaZ = collect_boruta_z_vs_shadow_max(\n",
    "    X_df=X_sel, y=y_sel,\n",
    "    rf_params=RF_PARAMS,\n",
    "    n_iters=N_ITERS, sample_frac=SAMPLE_FRAC, seed=SEED\n",
    ")\n",
    "\n",
    "# ---------- åˆå¹¶ Boruta åˆ¤å®šç»“æœï¼ˆç”¨äºç€è‰²ï¼‰ ----------\n",
    "decision_map = {}\n",
    "if 'selector' in globals(): \n",
    "    confirmed_feats = set(train_X_scaled.columns[np.where(selector.support_)[0]])  \n",
    "    tentative_feats_set = set(train_X_scaled.columns[np.where(selector.support_weak_)[0]])\n",
    "    for f in X_sel.columns:\n",
    "        if f in confirmed_feats:\n",
    "            decision_map[f] = \"Confirmed\"\n",
    "        elif f in tentative_feats_set:\n",
    "            decision_map[f] = \"Tentative\"\n",
    "        else:\n",
    "            decision_map[f] = \"Rejected\"\n",
    "else:\n",
    "    decision_map = {f: \"Tentative\" for f in X_sel.columns}\n",
    "\n",
    "long_borutaZ[\"decision\"] = long_borutaZ[\"feature\"].map(decision_map)\n",
    "\n",
    "# ---------- æŒ‰ä¸­ä½æ•° Z æ’åº ----------\n",
    "feat_order = (long_borutaZ.groupby(\"feature\")[\"z\"]\n",
    "              .median().sort_values(ascending=False).index.tolist())\n",
    "feats_for_plot = feat_order[::-1]\n",
    "\n",
    "# ---------- Ridge å±±è„Šå›¾å‡½æ•° ----------\n",
    "def ridgeline_plot_borutaZ(df_long, feats,\n",
    "                           x_label=\"Boruta Z-score (vs. Shadow Max)\",\n",
    "                           title=\"Feature Importance vs. Shadow Maximum (Standard Boruta)\"):\n",
    "    style_map = {\n",
    "        'Confirmed': {'lw': 1.8, 'alpha': 0.9},\n",
    "        'Tentative': {'lw': 1.2, 'alpha': 0.75},\n",
    "        'Rejected' : {'lw': 1.0, 'alpha': 0.55},\n",
    "    }\n",
    "    color_map = {\n",
    "        'Confirmed': '#1f77b4',\n",
    "        'Tentative': '#9467bd',\n",
    "        'Rejected' : '#7f7f7f',\n",
    "    }\n",
    "\n",
    "    all_z = df_long['z'].dropna().values\n",
    "    if len(all_z):\n",
    "        x_min = float(np.percentile(all_z, 0.5) - 0.5)\n",
    "        x_max = float(np.percentile(all_z, 99.5) + 0.5)\n",
    "    else:\n",
    "        x_min, x_max = -1.0, 1.0\n",
    "    xs = np.linspace(x_min, x_max, 350)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8.5, max(4.0, 0.44*len(feats)+2)))\n",
    "    y_offsets = np.arange(len(feats))\n",
    "\n",
    "    for y0, f in zip(y_offsets, feats):\n",
    "        sub = df_long.loc[df_long['feature'] == f]\n",
    "        dat = sub['z'].dropna().values\n",
    "        main_dec = sub['decision'].mode().iat[0] if len(sub) and not sub['decision'].mode().empty else 'Tentative'\n",
    "        style = style_map.get(main_dec, style_map['Tentative'])\n",
    "        c = color_map.get(main_dec, color_map['Tentative'])\n",
    "\n",
    "        if len(dat) < 5 or np.allclose(dat, dat[0]):\n",
    "            xm = float(np.mean(dat)) if len(dat) else 0.0\n",
    "            ax.plot([xm, xm], [y0, y0+0.85], lw=style['lw'], alpha=style['alpha'], color=c)\n",
    "            continue\n",
    "\n",
    "        kde = gaussian_kde(dat)\n",
    "        ys = kde(xs)\n",
    "        ys = ys / (ys.max() + 1e-12) * 0.8\n",
    "\n",
    "        ax.fill_between(xs, y0, y0 + ys, alpha=style['alpha'], color=c)\n",
    "        ax.plot(xs, y0 + ys, lw=style['lw'], color=c)\n",
    "        ax.plot(xs, [y0]*len(xs), lw=0.6, color='white')\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_yticks(y_offsets + 0.4)\n",
    "    ax.set_yticklabels(feats)\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.grid(True, axis='x', ls='--', alpha=0.35)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(-0.5, len(feats) - 0.1)\n",
    "\n",
    "    # å›¾ä¾‹\n",
    "    handles, labels = [], []\n",
    "    for k in ['Confirmed', 'Tentative', 'Rejected']:\n",
    "        if (df_long['decision'] == k).any():\n",
    "            handles.append(Line2D([0], [0], lw=style_map[k]['lw'], alpha=style_map[k]['alpha'], color=color_map[k]))\n",
    "            labels.append(k)\n",
    "    \n",
    "    if handles:\n",
    "        ax.legend(handles, labels, title=\"Decision Type\", loc='center left',\n",
    "                  bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "    plt.show()\n",
    "\n",
    "# ---------- ç”»å›¾ ----------\n",
    "ridgeline_plot_borutaZ(\n",
    "    df_long=long_borutaZ,\n",
    "    feats=feats_for_plot,\n",
    "    x_label=\"Boruta Z-score (vs. Shadow Max)\",\n",
    "    title=\"All Selected Features: Importance vs. Shadow Maximum\"\n",
    ")\n",
    "\n",
    "# ---------- å¯¼å‡ºæ±‡æ€»è¡¨ ----------\n",
    "borutaZ_summary = (long_borutaZ\n",
    "                   .groupby([\"feature\",\"decision\"])[\"z\"]\n",
    "                   .agg(['count','mean','median','std'])\n",
    "                   .reset_index()\n",
    "                   .sort_values(\"median\", ascending=False)).reset_index(drop=True)\n",
    "\n",
    "# æ·»åŠ hitç‡ç»Ÿè®¡\n",
    "hit_rate = (long_borutaZ\n",
    "            .groupby(\"feature\")\n",
    "            .apply(lambda x: (x[\"z\"] > 0).mean())\n",
    "            .rename(\"hit_rate\"))\n",
    "\n",
    "borutaZ_summary = borutaZ_summary.merge(\n",
    "    hit_rate.reset_index(), \n",
    "    on=\"feature\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š Boruta Z-score æ±‡æ€»ï¼ˆæ ‡å‡†æ–¹å¼ï¼švs. Shadow Maxï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Z > 0 è¡¨ç¤ºç‰¹å¾é‡è¦æ€§è¶…è¿‡shadowæœ€å¤§å€¼\")\n",
    "print(\"hit_rate = è¶…è¿‡shadowæœ€å¤§å€¼çš„è¿­ä»£æ¯”ä¾‹\")\n",
    "print(\"=\" * 70)\n",
    "print(borutaZ_summary.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c5cf3-8a06-4db4-a181-7b14513cf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬6æ­¥ï¼šåŸºäº Boruta é€‰å‡ºçš„ç‰¹å¾ï¼Œå®šä¹‰æ¨¡å‹é›†åˆ =====================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å®šä¹‰æ¨¡å‹é›†åˆ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "#å‡†å¤‡å¥½éªŒè¯æ•°æ®\n",
    "print(f\"ä½¿ç”¨ç‰¹å¾æ•°é‡: {X_train_for_models.shape[1]}\")\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {X_train_for_models.shape[0]}\")\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {X_val_for_models.shape[0]}\")\n",
    "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test_for_models.shape[0]}\")\n",
    "\n",
    "CLASS_WEIGHT = \"balanced\"\n",
    "\n",
    "# ---------- 6.1 CV ä¸è¯„åˆ†å™¨ ----------\n",
    "cv_5x3 = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE)\n",
    "auc_scorer = \"roc_auc\"\n",
    "\n",
    "# ---------- 6.2 å„æ¨¡å‹å®šä¹‰ ----------\n",
    "print(\"\\nå®šä¹‰æ¨¡å‹...\")\n",
    "\n",
    "# 1) Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    max_iter=5000, \n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "logit_grid = {\"C\": [0.0005, 0.001, 0.005]}\n",
    "logit_gs = GridSearchCV(logit, logit_grid, scoring=auc_scorer,\n",
    "                        cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                        error_score=\"raise\")\n",
    "\n",
    "# 2) LASSO / Elastic Net\n",
    "enet = LogisticRegression(\n",
    "    penalty=\"elasticnet\", \n",
    "    solver=\"saga\",\n",
    "    max_iter=5000,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "enet_grid = {\n",
    "    \"l1_ratio\": [0.7, 0.9, 1.0],\n",
    "    \"C\": [0.0005, 0.001, 0.005]\n",
    "}\n",
    "enet_gs = GridSearchCV(enet, enet_grid, scoring=auc_scorer,\n",
    "                       cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                       error_score=\"raise\")\n",
    "\n",
    "# 3) SVM (RBF)\n",
    "svm = SVC(\n",
    "    kernel=\"rbf\", \n",
    "    probability=True,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "svm_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "svm_gs = GridSearchCV(svm, svm_grid, scoring=auc_scorer,\n",
    "                      cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                      error_score=\"raise\")\n",
    "\n",
    "# 4) Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.5,\n",
    "    max_leaf_nodes=30,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    "    max_features=2,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf_grid = {\n",
    "    \"max_features\": [2, 3],\n",
    "    \"min_samples_leaf\": [10, 15, 20],\n",
    "    \"min_samples_split\": [10, 20]\n",
    "}\n",
    "rf_gs = GridSearchCV(rf, rf_grid, scoring=auc_scorer,\n",
    "                     cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                     error_score=\"raise\")\n",
    "\n",
    "# 5) XGBoost\n",
    "pos = int(np.sum(train_y == 1))\n",
    "neg = int(np.sum(train_y == 0))\n",
    "spw = (neg / max(pos, 1))\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=spw,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "xgb_grid = {\n",
    "    \"max_depth\": [2],\n",
    "    \"min_child_weight\": [80, 100, 150],   # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"gamma\": [5.0, 8.0, 10.0],            # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"reg_alpha\": [10.0, 15.0, 20.0],      # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"reg_lambda\": [10.0, 15.0, 20.0],     # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"colsample_bytree\": [0.2, 0.3],       # è¿›ä¸€æ­¥å‡å°‘\n",
    "    \"subsample\": [0.3, 0.4],              # è¿›ä¸€æ­¥å‡å°‘\n",
    "    \"learning_rate\": [0.005, 0.01],       # è¿›ä¸€æ­¥é™ä½\n",
    "    \"n_estimators\": [200, 300]\n",
    "}\n",
    "xgb_gs = GridSearchCV(xgb, xgb_grid, scoring=auc_scorer,\n",
    "                      cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                      error_score=\"raise\")\n",
    "\n",
    "# ---------- 6.3 æ¨¡å‹å­—å…¸ ----------\n",
    "models = {\n",
    "    \"Logistic\": logit_gs,\n",
    "    \"LASSO/ElasticNet\": enet_gs,\n",
    "    \"SVM\": svm_gs,\n",
    "    \"RandomForest\": rf_gs,\n",
    "    \"XGBoost\": xgb_gs\n",
    "}\n",
    "\n",
    "print(f\"\\n æ¨¡å‹å®šä¹‰å®Œæˆ\")\n",
    "print(f\"å°†å¯¹ä»¥ä¸‹ {len(models)} ä¸ªæ¨¡å‹è¿›è¡Œ 5Ã—3 CV è°ƒå‚ï¼š\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"   {i}. {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬6.4æ­¥ï¼šè¾…åŠ©å‡½æ•°å®šä¹‰ =====================\n",
    "# è¿™äº›å‡½æ•°å°†åœ¨ç¬¬7æ­¥çš„æ¨¡å‹è¯„ä¼°ä¸­ä½¿ç”¨\n",
    "def to_score(estimator, X):\n",
    "    \"\"\"\n",
    "    ä»ä»»æ„sklearnåˆ†ç±»å™¨è·å–æ­£ç±»æ¦‚ç‡åˆ†æ•°\n",
    "    å…¼å®¹Pipelineã€CalibratedClassifierCVã€SVMç­‰\n",
    "    \"\"\"\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        proba = estimator.predict_proba(X)\n",
    "        if proba.ndim == 1:\n",
    "            return proba\n",
    "        elif proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "        return np.squeeze(proba)\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        # å‡çº§ï¼šz-score + sigmoidï¼Œç¡®ä¿è¾“å‡ºä¸ºæ¦‚ç‡\n",
    "        scores = estimator.decision_function(X)\n",
    "        mu = np.mean(scores)\n",
    "        sigma = np.std(scores) + 1e-12\n",
    "        return expit((scores - mu) / sigma)\n",
    "    else:\n",
    "        return estimator.predict(X).astype(float)\n",
    "\n",
    "\n",
    "def youden_threshold(y_true, y_score):\n",
    "    \"\"\"\n",
    "    æ ¹æ®YoudenæŒ‡æ•°ï¼ˆçµæ•åº¦+ç‰¹å¼‚åº¦-1çš„æœ€å¤§å€¼ï¼‰è®¡ç®—æœ€ä¼˜åˆ†ç±»é˜ˆå€¼\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, çœŸå®æ ‡ç­¾\n",
    "    y_score : array-like, é¢„æµ‹æ¦‚ç‡åˆ†æ•°\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    threshold : float, æœ€ä¼˜é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_index)\n",
    "    optimal_threshold = float(thresholds[optimal_idx])\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def binary_metrics_at(y_true, y_score, threshold):\n",
    "    \"\"\"\n",
    "    åœ¨ç»™å®šé˜ˆå€¼ä¸‹è®¡ç®—äºŒåˆ†ç±»çš„æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, çœŸå®æ ‡ç­¾ (0/1)\n",
    "    y_score : array-like, é¢„æµ‹æ¦‚ç‡åˆ†æ•°\n",
    "    threshold : float, åˆ†ç±»é˜ˆå€¼\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    accuracy, sensitivity, specificity, ppv, npv : float\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    \n",
    "    # æ ¹æ®é˜ˆå€¼è¿›è¡Œåˆ†ç±»\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    \n",
    "    # è®¡ç®—æ··æ·†çŸ©é˜µçš„å„ä¸ªç»„æˆéƒ¨åˆ†\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    # è®¡ç®—å„é¡¹æŒ‡æ ‡\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # çµæ•åº¦\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0  # ç‰¹å¼‚åº¦\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0          # é˜³æ€§é¢„æµ‹å€¼\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0          # é˜´æ€§é¢„æµ‹å€¼\n",
    "    \n",
    "    return float(accuracy), float(sensitivity), float(specificity), float(ppv), float(npv)\n",
    "\n",
    "def auc_ci_bootstrap(y_true, y_score, n_boot=1000, seed=42, alpha=0.95):\n",
    "    \"\"\"Bootstrap è®¡ç®— AUC ç½®ä¿¡åŒºé—´\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    \n",
    "    if len(y_true) != len(y_score):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # ç±»åˆ«æ£€æŸ¥\n",
    "    pos_idx = np.where(y_true == 1)[0]\n",
    "    neg_idx = np.where(y_true == 0)[0]\n",
    "    \n",
    "    if len(pos_idx) < 2 or len(neg_idx) < 2 or n < 10:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            return float(auc), float(auc)\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # åˆ†å±‚ Bootstrap\n",
    "    aucs = []\n",
    "    for _ in range(n_boot):\n",
    "        try:\n",
    "            boot_pos = rng.choice(pos_idx, size=len(pos_idx), replace=True)\n",
    "            boot_neg = rng.choice(neg_idx, size=len(neg_idx), replace=True)\n",
    "            boot_idx = np.concatenate([boot_pos, boot_neg])\n",
    "            aucs.append(roc_auc_score(y_true[boot_idx], y_score[boot_idx]))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(aucs) == 0:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            return float(auc), float(auc)\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # ç½®ä¿¡åŒºé—´\n",
    "    lower_p = ((1.0 - alpha) / 2.0) * 100\n",
    "    upper_p = (alpha + (1.0 - alpha) / 2.0) * 100\n",
    "    return float(np.percentile(aucs, lower_p)), float(np.percentile(aucs, upper_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬7æ­¥ï¼šè®­ç»ƒä¸è¯„ä¼° ============================\n",
    "\n",
    "# ===================== 7.1 CVè°ƒå‚ =====================\n",
    "fit_models = {}\n",
    "cv_report = {}\n",
    "grid_search_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"é˜¶æ®µ1ï¼šCVè°ƒå‚\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, est in models.items():\n",
    "    try:\n",
    "        est_ = clone(est)\n",
    "        est_.fit(X_train_for_models, train_y)\n",
    "        \n",
    "        best_est = est_.best_estimator_ if hasattr(est_, \"best_estimator_\") else est_\n",
    "        best_score = est_.best_score_ if hasattr(est_, \"best_score_\") else np.nan\n",
    "        \n",
    "        fit_models[name] = best_est\n",
    "        grid_search_results[name] = est_\n",
    "        cv_report[name] = best_score\n",
    "        \n",
    "        print(f\"[âœ“] {name:20s} | CV AUC = {best_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âœ—] {name:20s} | å¤±è´¥: {e}\")\n",
    "        fit_models[name] = None\n",
    "        grid_search_results[name] = None\n",
    "        cv_report[name] = np.nan\n",
    "\n",
    "\n",
    "# ===================== 7.2 éªŒè¯é›†é€‰æ‹©æœ€ä¼˜æ¨¡å‹ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ2ï¼šéªŒè¯é›†é€‰æ‹©æœ€ä¼˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "val_aucs = {}\n",
    "for name, est in fit_models.items():\n",
    "    if est is None:\n",
    "        val_aucs[name] = np.nan\n",
    "        continue\n",
    "    try:\n",
    "        y_val_prob = to_score(est, X_val_for_models)\n",
    "        val_aucs[name] = roc_auc_score(val_y, y_val_prob)\n",
    "        print(f\"[âœ“] {name:20s} | Val AUC = {val_aucs[name]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âœ—] {name:20s} | å¤±è´¥: {e}\")\n",
    "        val_aucs[name] = np.nan\n",
    "\n",
    "best_model_name = max(val_aucs.items(), key=lambda x: x[1])[0]\n",
    "best_model_params = fit_models[best_model_name]\n",
    "print(f\"\\nğŸ† æœ€ä¼˜æ¨¡å‹: {best_model_name} (Val AUC = {val_aucs[best_model_name]:.4f})\")\n",
    "\n",
    "\n",
    "# ===================== 7.3 ç¡®å®šæœ€ä¼˜æ¨¡å‹ï¼ˆä¸é‡è®­ç»ƒï¼‰ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ3ï¼šç¡®å®šæœ€ä¼˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æœ€ä¼˜æ¨¡å‹ç›´æ¥ä½¿ç”¨ 7.1 è®­ç»ƒå¥½çš„\n",
    "best_model = fit_models[best_model_name]\n",
    "\n",
    "# æ ¡å‡†ï¼ˆåªç”¨è®­ç»ƒé›†ï¼‰\n",
    "calibrated_best = CalibratedClassifierCV(estimator=clone(best_model), method=\"sigmoid\", cv=5)\n",
    "calibrated_best.fit(X_train_for_models, train_y)\n",
    "\n",
    "# é˜ˆå€¼ï¼ˆåªç”¨è®­ç»ƒé›†ï¼‰\n",
    "y_train_prob = to_score(calibrated_best, X_train_for_models)\n",
    "optimal_threshold = youden_threshold(train_y, y_train_prob)\n",
    "\n",
    "print(f\" æœ€ä¼˜æ¨¡å‹: {best_model_name}\")\n",
    "print(f\" Youdené˜ˆå€¼ = {optimal_threshold:.4f}\")\n",
    "\n",
    "# ===================== 7.4 æ‰€æœ‰æ¨¡å‹è¯„ä¼°ï¼ˆTrain/Val/Test ç‹¬ç«‹ï¼‰ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ6ï¼šæ‰€æœ‰æ¨¡å‹è¯„ä¼°\")\n",
    "print(\"=\" * 70)\n",
    "calibrated_models_all = {}\n",
    "all_results = []\n",
    "\n",
    "for name, model in fit_models.items():\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        calibrated = CalibratedClassifierCV(estimator=clone(model), method=\"sigmoid\", cv=5)\n",
    "        calibrated.fit(X_train_for_models, train_y)\n",
    "        calibrated_models_all[name] = calibrated\n",
    "    except:\n",
    "        calibrated = model\n",
    "    \n",
    "    try:\n",
    "        train_prob_for_thr = to_score(calibrated, X_train_for_models)\n",
    "        threshold = youden_threshold(train_y, train_prob_for_thr)\n",
    "    except:\n",
    "        threshold = 0.5\n",
    "    \n",
    "    datasets = [\n",
    "        (\"Train\", X_train_for_models, train_y),\n",
    "        (\"Val\", X_val_for_models, val_y),\n",
    "        (\"Test\", X_test_for_models, test_y)\n",
    "    ]\n",
    "    \n",
    "    for split_name, Xs, yss in datasets:\n",
    "        try:\n",
    "            prob = to_score(calibrated, Xs)\n",
    "            auc_val = roc_auc_score(yss, prob)\n",
    "            ci_lo, ci_hi = auc_ci_bootstrap(yss, prob, n_boot=800, seed=RANDOM_STATE)\n",
    "            acc, sen, spe, ppv, npv = binary_metrics_at(yss, prob, threshold)\n",
    "            \n",
    "            # è®¡ç®—F1åˆ†æ•°\n",
    "            y_pred = (prob >= threshold).astype(int)\n",
    "            f1 = f1_score(yss, y_pred)\n",
    "            \n",
    "            all_results.append({\n",
    "                \"Model\": name, \"Dataset\": split_name, \"AUC\": auc_val,\n",
    "                \"AUC 95%CI\": f\"[{ci_lo:.3f}, {ci_hi:.3f}]\", \"Threshold\": threshold,\n",
    "                \"Accuracy\": acc, \"Precision\": ppv, \"Recall\": sen,  # PPV=Precision, Sensitivity=Recall\n",
    "                \"F1 Score\": f1, \"Specificity\": spe, \"NPV\": npv\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[âœ—] {name} @ {split_name}: {e}\")\n",
    "\n",
    "df_all_results = pd.DataFrame(all_results)\n",
    "\n",
    "# AUC å®½æ ¼å¼å¯¹æ¯”\n",
    "pivot_auc = df_all_results.pivot(index='Model', columns='Dataset', values='AUC')\n",
    "pivot_auc = pivot_auc[['Train', 'Val', 'Test']]\n",
    "pivot_auc['Train-Test Gap'] = pivot_auc['Train'] - pivot_auc['Test']\n",
    "pivot_auc = pivot_auc.sort_values('Test', ascending=False)\n",
    "\n",
    "print(\"\\n AUC å¯¹æ¯”:\")\n",
    "display(pivot_auc.round(4))\n",
    "\n",
    "# æ‰€æœ‰æ¨¡å‹ä¸‰é›†è¯¦ç»†æ€§èƒ½\n",
    "print(\"\\n æ‰€æœ‰æ¨¡å‹å®Œæ•´æ€§èƒ½è¯¦æƒ…:\")\n",
    "df_display = df_all_results.copy()\n",
    "df_display = df_display.sort_values(['Model', 'Dataset'], \n",
    "                                     key=lambda x: x.map({'Train': 0, 'Val': 1, 'Test': 2}) if x.name == 'Dataset' else x)\n",
    "df_display = df_display[['Model', 'Dataset', 'AUC', 'Accuracy', 'AUC 95%CI', 'Threshold', \n",
    "                          'Precision', 'Recall', 'F1 Score', 'Specificity', 'NPV']]\n",
    "display(df_display.round(4))\n",
    "\n",
    "# æœ€ä¼˜æ¨¡å‹\n",
    "df_test = df_all_results[df_all_results['Dataset'] == 'Test'].sort_values('AUC', ascending=False)\n",
    "best_model_name = df_test.iloc[0]['Model']\n",
    "print(f\"\\n æœ€ä¼˜æ¨¡å‹: {best_model_name} (Test AUC = {df_test.iloc[0]['AUC']:.4f})\")\n",
    "\n",
    "\n",
    "# ===================== 7.5 æ‹Ÿåˆç¨‹åº¦å¯è§†åŒ– =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ7ï¼šæ‹Ÿåˆç¨‹åº¦å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # æ”¹ä¸º2åˆ—\n",
    "\n",
    "# --- å›¾1: åˆ†ç»„æŸ±çŠ¶å›¾ ---\n",
    "ax1 = axes[0]\n",
    "models_order = pivot_auc.index.tolist()\n",
    "x = np.arange(len(models_order))\n",
    "width = 0.25\n",
    "\n",
    "train_vals = [pivot_auc.loc[m, 'Train'] for m in models_order]\n",
    "val_vals = [pivot_auc.loc[m, 'Val'] for m in models_order]\n",
    "test_vals = [pivot_auc.loc[m, 'Test'] for m in models_order]\n",
    "\n",
    "bars1 = ax1.bar(x - width, train_vals, width, label='Train', color='#3498db', alpha=0.8)\n",
    "bars2 = ax1.bar(x, val_vals, width, label='Val', color='#2ecc71', alpha=0.8)\n",
    "bars3 = ax1.bar(x + width, test_vals, width, label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AUC', fontsize=12)\n",
    "ax1.set_title('Train / Val / Test AUC Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_order, rotation=30, ha='right')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "ax1.axhline(y=0.7, color='gray', linestyle='--', alpha=0.5, label='AUC=0.7')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# --- å›¾2: Train-Test Gap æŸ±çŠ¶å›¾ ---\n",
    "ax2 = axes[1]\n",
    "gaps = [pivot_auc.loc[m, 'Train-Test Gap'] for m in models_order]\n",
    "colors = ['#27ae60' if g < 0.05 else '#f39c12' if g < 0.10 else '#e67e22' if g < 0.15 else '#c0392b' for g in gaps]\n",
    "\n",
    "bars_gap = ax2.bar(x, gaps, width=0.5, color=colors, alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax2.set_ylabel('Train - Test Gap', fontsize=12)\n",
    "ax2.set_title('Overfitting Assessment (Train-Test Gap)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_order, rotation=30, ha='right')\n",
    "ax2.axhline(y=0.05, color='green', linestyle='--', alpha=0.7, linewidth=1.5, label='Good (<0.05)')\n",
    "ax2.axhline(y=0.10, color='orange', linestyle='--', alpha=0.7, linewidth=1.5, label='Acceptable (<0.10)')\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', alpha=0.7, linewidth=1.5, label='Concern (>0.15)')\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.set_ylim(0, max(gaps) * 1.3)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, gap in zip(bars_gap, gaps):\n",
    "    ax2.annotate(f'{gap:.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n å¯è§†åŒ–å·²ä¿å­˜: overfitting_analysis.png\")\n",
    "\n",
    "# ===================== 7.6 åˆ›å»ºåç»­å˜é‡ & ä¿å­˜æ¨¡å‹ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ8ï¼šä¿å­˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ä½¿ç”¨ 7.3 ä¸­åˆ›å»ºçš„æ ¡å‡†æ¨¡å‹ï¼ˆä¸å†å¼•ç”¨ calibrated_finalï¼‰\n",
    "# calibrated_best å·²åœ¨ 7.3 ä¸­å®šä¹‰\n",
    "best_est = fit_models[best_model_name]\n",
    "calibrated_models = {best_model_name: calibrated_best}\n",
    "y_test_prob = to_score(calibrated_best, X_test_for_models)\n",
    "thr_star = optimal_threshold\n",
    "has_val = True  # ä½ æœ‰éªŒè¯é›†\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "import pickle\n",
    "test_auc_value = df_test[df_test['Model']==best_model_name]['AUC'].values[0]\n",
    "\n",
    "# è·å–æœ€ä¼˜å‚æ•°\n",
    "if grid_search_results.get(best_model_name) is not None:\n",
    "    best_params = grid_search_results[best_model_name].best_params_\n",
    "else:\n",
    "    best_params = None\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': calibrated_best,\n",
    "    'model_name': best_model_name,\n",
    "    'features': list(X_train_for_models.columns),\n",
    "    'threshold': optimal_threshold,\n",
    "    'best_params': best_params,\n",
    "    'metadata': {'train_size': len(X_train_for_models), 'test_auc': test_auc_value, 'random_state': RANDOM_STATE}\n",
    "}\n",
    "\n",
    "with open(\"best_model_complete.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(f\" æ¨¡å‹å·²ä¿å­˜: best_model_complete.pkl\")\n",
    "print(f\" æœ€ä¼˜æ¨¡å‹: {best_model_name}\")\n",
    "print(f\" é˜ˆå€¼: {optimal_threshold:.4f} | æµ‹è¯•AUC: {test_auc_value:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e88d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 7.7 ROC æ›²çº¿ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰ =====================\n",
    "def plot_roc_panel(models_dict):\n",
    "    # å±•ç¤º Train, Val, Test ä¸‰ä¸ªæ•°æ®é›†\n",
    "    sets = [\n",
    "        (\"Train\", X_train_for_models, train_y),\n",
    "        (\"Val\", X_val_for_models, val_y),\n",
    "        (\"Test\", X_test_for_models, test_y)\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for ax, (set_name, Xs, ys) in zip(axes, sets):\n",
    "        for name, est in models_dict.items():\n",
    "            y_prob = to_score(est, Xs)\n",
    "            fpr, tpr, _ = roc_curve(ys, y_prob)\n",
    "            auc_val = roc_auc_score(ys, y_prob)\n",
    "            ax.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        ax.set_title(f\"ROC â€” {set_name}\", fontsize=13)\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend(loc=\"lower right\", fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    fig.suptitle(\"ROC Curves by Dataset (All Calibrated Models)\", y=1.02, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curves_all_models.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_panel(calibrated_models_all)\n",
    "\n",
    "# ===================== 7.8 DCAï¼ˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ï¼‰ =====================\n",
    "def decision_curve(y_true, y_prob, thresholds=None):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(y_prob).astype(float)\n",
    "    n = len(y)\n",
    "    prev = y.mean()\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.60, 30)\n",
    "    out = []\n",
    "    for pt in thresholds:\n",
    "        pred = (p >= pt).astype(int)\n",
    "        tp = np.sum((pred == 1) & (y == 1))\n",
    "        fp = np.sum((pred == 1) & (y == 0))\n",
    "        nb_model = (tp / n) - (fp / n) * (pt / (1 - pt))\n",
    "        nb_all = prev - (1 - prev) * (pt / (1 - pt))\n",
    "        nb_none = 0.0\n",
    "        out.append((pt, nb_model, nb_all, nb_none))\n",
    "    return pd.DataFrame(out, columns=[\"pt\", \"NB_model\", \"NB_all\", \"NB_none\"])\n",
    "\n",
    "def plot_dca(df, title=\"\", mark_pt=None, db_name=\"MIMIC-IV\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(df[\"pt\"], df[\"NB_model\"], lw=2, label=\"Model\")\n",
    "    ax.plot(df[\"pt\"], df[\"NB_all\"], lw=2, color=\"gray\", label=\"Treat All\")\n",
    "    ax.plot(df[\"pt\"], df[\"NB_none\"], lw=2, ls=\"--\", color=\"gray\", label=\"Treat None\")\n",
    "\n",
    "    if mark_pt is not None:\n",
    "        row = df.iloc[(df[\"pt\"] - mark_pt).abs().argmin()]\n",
    "        ax.scatter([row[\"pt\"]], [row[\"NB_model\"]], s=45, label=f\"thrâ‰ˆ{row['pt']:.2f}\", color=\"red\")\n",
    "\n",
    "    ax.set_xlabel(\"Threshold Probability\")\n",
    "    ax.set_ylabel(\"Net Benefit\")\n",
    "    ax.set_title(f\"{title} â€” {db_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # å›ºå®š y è½´èŒƒå›´\n",
    "    ax.set_ylim(-0.05, 0.30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒé›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äº DCAï¼‰\n",
    "y_train_prob = to_score(calibrated_best, X_train_for_models)\n",
    "\n",
    "# è®­ç»ƒé›† DCA\n",
    "plot_dca(decision_curve(train_y, y_train_prob),\n",
    "         title=f\"DCA â€” Calibrated {best_model_name}\", mark_pt=thr_star, db_name=\"Train\")\n",
    "\n",
    "# æµ‹è¯•é›† DCA\n",
    "plot_dca(decision_curve(test_y, y_test_prob),\n",
    "         title=f\"DCA â€” Calibrated {best_model_name}\", mark_pt=thr_star, db_name=\"Test\")\n",
    "\n",
    "# æ‰¾åˆ°è“çº¿ç©¿è¿‡0çš„ç²¾ç¡®é˜ˆå€¼\n",
    "dca_test = decision_curve(test_y, y_test_prob)\n",
    "valid_range = dca_test[dca_test['NB_model'] > 0]['pt']\n",
    "print(f\"æ¨¡å‹å‡€æ”¶ç›Š>0çš„é˜ˆå€¼èŒƒå›´: {valid_range.min():.2f} - {valid_range.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddca48-01be-4053-abef-7bab6551a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬8æ­¥ï¼šSHAP è§£é‡Šåˆ†æ =====================\n",
    "print(\"=\" * 70)\n",
    "print(\"ç¬¬8æ­¥ï¼šSHAP è§£é‡Šåˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------------- 8.1 å‡†å¤‡å·¥ä½œï¼šä½¿ç”¨Borutaç­›é€‰åçš„ç‰¹å¾ ----------------\n",
    "features_for_shap = list(X_train_for_models.columns)\n",
    "print(f\"\\nâœ… Borutaç­›é€‰ç‰¹å¾æ•°: {len(features_for_shap)}\")\n",
    "print(f\"   - ç¡®è®¤ç‰¹å¾: {len(selected_features)}\")\n",
    "print(f\"   - å¾…å®šç‰¹å¾: {len(tentative_features)}\")\n",
    "\n",
    "# è·å–æœ€ä¼˜æ¨¡å‹ï¼ˆå·²æ ¡å‡†ï¼‰\n",
    "shap_model = calibrated_best\n",
    "print(f\"\\nâœ… SHAP è§£é‡Šæ¨¡å‹: {best_model_name} (å·²æ ¡å‡†)\")\n",
    "\n",
    "# ---------------- 8.2 æ„å»º SHAP Explainer ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ„å»º SHAP Explainer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# èƒŒæ™¯æ•°æ®ï¼šä»è®­ç»ƒé›†ä¸­é‡‡æ ·\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "n_background = min(200, len(X_train_for_models))\n",
    "bg_idx = rng.choice(len(X_train_for_models), size=n_background, replace=False)\n",
    "X_background = X_train_for_models.iloc[bg_idx].copy()\n",
    "\n",
    "print(f\"èƒŒæ™¯æ ·æœ¬æ•°: {n_background} (æ¥è‡ªè®­ç»ƒé›†)\")\n",
    "\n",
    "# é¢„æµ‹å‡½æ•°\n",
    "def predict_proba_class1(X):\n",
    "    \"\"\"SHAPä¸“ç”¨ï¼šè¿”å›æ­£ç±»æ¦‚ç‡\"\"\"\n",
    "    return to_score(shap_model, X)\n",
    "\n",
    "# åˆ›å»º Explainer\n",
    "try:\n",
    "    explainer = shap.Explainer(predict_proba_class1, X_background)\n",
    "    print(\"âœ… SHAP Explainer åˆ›å»ºæˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä½¿ç”¨å‡½æ•°å¼ Explainer å¤±è´¥: {e}\")\n",
    "    print(\"   å°è¯•ä½¿ç”¨ KernelExplainer...\")\n",
    "    explainer = shap.KernelExplainer(predict_proba_class1, X_background)\n",
    "    print(\"âœ… SHAP KernelExplainer åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "# ---------------- 8.2.1 è®¡ç®—æµ‹è¯•é›†çš„ SHAP å€¼ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"è®¡ç®— SHAP å€¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åªè®¡ç®—æµ‹è¯•é›† SHAP\n",
    "print(f\"è®¡ç®—æµ‹è¯•é›† SHAP å€¼ (n={len(X_test_for_models)})...\")\n",
    "sv_test = explainer(X_test_for_models)\n",
    "\n",
    "# æå–æ•°ç»„\n",
    "def extract_shap_array(sv):\n",
    "    if hasattr(sv, 'values'):\n",
    "        arr = sv.values\n",
    "    else:\n",
    "        arr = sv\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[:, :, 1]\n",
    "    return arr\n",
    "\n",
    "shap_array_test = extract_shap_array(sv_test)\n",
    "print(f\"âœ… æµ‹è¯•é›† SHAP å€¼: {shap_array_test.shape}\")\n",
    "\n",
    "# ç»Ÿä¸€å˜é‡å\n",
    "sv = sv_test\n",
    "X_explain = X_test_for_models\n",
    "shap_values = sv_test\n",
    "X_shap = X_test_for_models\n",
    "y_shap = test_y\n",
    "shap_array = shap_array_test\n",
    "which_split = \"Test\"\n",
    "\n",
    "# ---------------- 8.3 å…¨å±€ç‰¹å¾é‡è¦æ€§ï¼šBeeswarm å›¾ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.3 å…¨å±€ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "plt.figure(figsize=(10, max(6, 0.35 * len(features_for_shap))))\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    features=X_shap,\n",
    "    feature_names=features_for_shap,\n",
    "    show=False,\n",
    "    max_display=len(features_for_shap)\n",
    ")\n",
    "plt.title(f\"SHAP Summary Plot â€” {best_model_name} (Critically Ill Patients)\", \n",
    "          fontsize=14, pad=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- 8.4 Dependence Plotsï¼ˆTop 9 ç‰¹å¾ï¼‰ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.4 SHAP Dependence Plots\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è®¡ç®—ç‰¹å¾é‡è¦æ€§å¹¶é€‰æ‹© Top 9\n",
    "mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "importance_df = pd.Series(mean_abs_shap, index=features_for_shap).sort_values(ascending=False)\n",
    "\n",
    "n_top = min(15, len(features_for_shap))\n",
    "top_features = importance_df.head(n_top).index.tolist()\n",
    "\n",
    "print(f\"ç»˜åˆ¶ Top {n_top} ç‰¹å¾çš„ä¾èµ–å›¾:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat} (Mean |SHAP|={importance_df[feat]:.4f})\")\n",
    "\n",
    "# å¸ƒå±€\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(n_top / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "axes = axes.flatten() if n_top > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    ax = axes[idx]\n",
    "    feat_idx = features_for_shap.index(feature)\n",
    "    x_values = X_shap[feature].values\n",
    "    y_values = shap_array[:, feat_idx]\n",
    "    \n",
    "    valid_mask = np.isfinite(x_values) & np.isfinite(y_values)\n",
    "    x_values = x_values[valid_mask]\n",
    "    y_values = y_values[valid_mask]\n",
    "    \n",
    "    if len(x_values) > 0:\n",
    "        vmin_feat = np.percentile(x_values, 1)\n",
    "        vmax_feat = np.percentile(x_values, 99)\n",
    "        norm_feat = mpl.colors.Normalize(vmin=vmin_feat, vmax=vmax_feat)\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            x_values, y_values,\n",
    "            c=x_values,\n",
    "            cmap='RdYlBu_r',\n",
    "            norm=norm_feat,\n",
    "            s=25,\n",
    "            alpha=0.6,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        cbar_ax = inset_axes(\n",
    "            ax,\n",
    "            width=\"3%\",\n",
    "            height=\"80%\",\n",
    "            loc='center right',\n",
    "            bbox_to_anchor=(0.18, 0, 1, 1),\n",
    "            bbox_transform=ax.transAxes,\n",
    "            borderpad=0\n",
    "        )\n",
    "        plt.colorbar(scatter, cax=cbar_ax)\n",
    "        \n",
    "        ax.set_xlabel(feature, fontsize=10)\n",
    "        ax.set_ylabel('SHAP value', fontsize=10)\n",
    "        ax.set_title(f'Dependence: {feature}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3, linestyle='--')\n",
    "        ax.axhline(0, color='black', linewidth=0.8, linestyle='-', alpha=0.3)\n",
    "\n",
    "for idx in range(n_top, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.suptitle(f'SHAP Dependence Plots â€” {best_model_name} (Critically Ill Patients)',\n",
    "             y=1.00, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- 8.5 ä¸ªä½“è§£é‡Šï¼šWaterfall å›¾ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.5 ä¸ªä½“è§£é‡Šï¼ˆWaterfall Plotsï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba = predict_proba_class1(X_shap)\n",
    "idx_high = np.argmax(y_pred_proba)\n",
    "idx_low = np.argmin(y_pred_proba)\n",
    "\n",
    "print(f\"æœ€é«˜æ­»äº¡é£é™©æ ·æœ¬: ç´¢å¼•={idx_high}, é¢„æµ‹æ¦‚ç‡={y_pred_proba[idx_high]:.4f}\")\n",
    "print(f\"æœ€ä½æ­»äº¡é£é™©æ ·æœ¬: ç´¢å¼•={idx_low}, é¢„æµ‹æ¦‚ç‡={y_pred_proba[idx_low]:.4f}\")\n",
    "\n",
    "def plot_waterfall_native(shap_vals, X_data, sample_idx, title, shap_arr, max_display=15):\n",
    "    try:\n",
    "        if hasattr(shap_vals, 'base_values'):\n",
    "            base_value = shap_vals.base_values[sample_idx]\n",
    "        else:\n",
    "            base_value = explainer.expected_value\n",
    "            if isinstance(base_value, (list, np.ndarray)):\n",
    "                base_value = base_value[1] if len(base_value) > 1 else base_value[0]\n",
    "        \n",
    "        explanation = shap.Explanation(\n",
    "            values=shap_arr[sample_idx],\n",
    "            base_values=base_value,\n",
    "            data=X_data.iloc[sample_idx].values,\n",
    "            feature_names=features_for_shap\n",
    "        )\n",
    "        \n",
    "        plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "        plt.rcParams['font.size'] = 10\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, max(10, 0.5 * max_display)))\n",
    "        shap.plots.waterfall(explanation, max_display=max_display, show=False)\n",
    "        plt.gcf().suptitle(title, fontsize=14, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nè¯¥æ ·æœ¬çš„è¯¦ç»†ç‰¹å¾å€¼:\")\n",
    "        sample_data = X_data.iloc[sample_idx]\n",
    "        shap_vals_sample = shap_arr[sample_idx]\n",
    "        \n",
    "        abs_shap = np.abs(shap_vals_sample)\n",
    "        sorted_idx = np.argsort(abs_shap)[::-1][:max_display]\n",
    "        \n",
    "        print(f\"{'ç‰¹å¾å':<25} {'ç‰¹å¾å€¼':>12} {'SHAPå€¼':>12}\")\n",
    "        print(\"-\" * 52)\n",
    "        for idx in sorted_idx:\n",
    "            feat_name = features_for_shap[idx]\n",
    "            feat_val = sample_data[feat_name]\n",
    "            shap_val = shap_vals_sample[idx]\n",
    "            print(f\"{feat_name:<25} {feat_val:>12.4f} {shap_val:>12.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Waterfall å›¾ç»˜åˆ¶å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é«˜æ­»äº¡é£é™©æ ·æœ¬ Waterfall å›¾\")\n",
    "print(\"=\" * 70)\n",
    "plot_waterfall_native(\n",
    "    shap_values, X_shap, idx_high,\n",
    "    f\"High Mortality Risk Patient (Prob={y_pred_proba[idx_high]:.3f}) â€” {best_model_name}\",\n",
    "    shap_array,\n",
    "    max_display=15\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ä½æ­»äº¡é£é™©æ ·æœ¬ Waterfall å›¾\")\n",
    "print(\"=\" * 70)\n",
    "plot_waterfall_native(\n",
    "    shap_values, X_shap, idx_low,\n",
    "    f\"Low Mortality Risk Patient (Prob={y_pred_proba[idx_low]:.3f}) â€” {best_model_name}\",\n",
    "    shap_array,\n",
    "    max_display=15\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ä¸ªä½“è§£é‡Šå®Œæˆ\")\n",
    "\n",
    "# ---------------- 8.6 ç‰¹å¾é‡è¦æ€§æ±‡æ€»è¡¨ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.6 ç‰¹å¾é‡è¦æ€§æ±‡æ€»\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Feature': features_for_shap,\n",
    "    'Mean_|SHAP|': np.abs(shap_array).mean(axis=0),\n",
    "    'Std_SHAP': shap_array.std(axis=0),\n",
    "    'Mean_SHAP': shap_array.mean(axis=0)\n",
    "})\n",
    "\n",
    "summary_df = summary_df.sort_values('Mean_|SHAP|', ascending=False).reset_index(drop=True)\n",
    "summary_df['Rank'] = range(1, len(summary_df) + 1)\n",
    "\n",
    "print(f\"\\nç‰¹å¾é‡è¦æ€§æ’åºï¼ˆåŸºäºæµ‹è¯•é›†ï¼Œn={len(X_shap)}ï¼‰:\")\n",
    "display(summary_df[['Rank', 'Feature', 'Mean_|SHAP|', 'Mean_SHAP', 'Std_SHAP']].round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Force-like å¤šæ ·æœ¬æŠ˜çº¿å›¾ï¼ˆç¨³å¥ç‰ˆï¼Œé€‚é…æ ¡å‡†åæ¨¡å‹ï¼‰ ========\n",
    "def _proba_from_any(est, X_df):\n",
    "    \"\"\"\n",
    "    ä»ä»»æ„äºŒåˆ†ç±»ä¼°è®¡å™¨æ‹¿â€œæ­£ç±»æ¦‚ç‡â€æ ·å¼åˆ†æ•°ã€‚\n",
    "    å…¼å®¹ CalibratedClassifierCV / Pipeline / SVM(decision_function) / åªæœ‰ predict çš„æ¨¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    if hasattr(est, \"predict_proba\"):\n",
    "        p = est.predict_proba(X_df)\n",
    "        return p[:, 1] if p.ndim == 2 and p.shape[1] >= 2 else np.squeeze(p).astype(float)\n",
    "    if hasattr(est, \"decision_function\"):\n",
    "        s = np.asarray(est.decision_function(X_df), dtype=float)\n",
    "        # z-score åè¿‡ sigmoidï¼Œé¿å…å°ºåº¦å·®å¼‚\n",
    "        mu, sd = float(np.mean(s)), float(np.std(s) + 1e-12)\n",
    "        from scipy.special import expit\n",
    "        return expit((s - mu) / sd)\n",
    "    pred = np.squeeze(est.predict(X_df)).astype(float)\n",
    "    if set(np.unique(pred)).issubset({0.0, 1.0}):\n",
    "        eps = 1e-6\n",
    "        return pred * (1 - 2 * eps) + eps\n",
    "    # æœ€åå…œåº•ï¼šmin-max åˆ°(0,1)\n",
    "    pmin, pmax = np.min(pred), np.max(pred)\n",
    "    return (pred - pmin) / (pmax - pmin + 1e-12)\n",
    "\n",
    "\n",
    "def plot_force_like_lines(\n",
    "    sv, X_df, title=\"\",\n",
    "    top_n=10,\n",
    "    smooth=0,                # æ»‘åŠ¨çª—å£å¹³æ»‘ï¼ˆ>1 å¼€å¯ï¼‰\n",
    "    legend_out=True,\n",
    "    order_scores=None,       # è‹¥æä¾›åˆ™æŒ‰æ¦‚ç‡ç”±é«˜åˆ°ä½æ’åº\n",
    "    fixed_top_idx=None,      # è‹¥æä¾›åˆ™å¼ºåˆ¶ä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„åˆ—ç´¢å¼•\n",
    "    ylim=None,\n",
    "    max_legend=10,           # å›¾ä¾‹æœ€å¤šæ˜¾ç¤ºå¤šå°‘æ¡\n",
    "    clip_pct=(0.5, 99.5)     # y æŒ‰åˆ†ä½è£å‰ªï¼ŒæŠ‘åˆ¶æç«¯ç‚¹\n",
    "):\n",
    "    \"\"\"\n",
    "    ç”¨æŠ˜çº¿æ–¹å¼å¤ç° Fig.6 é£æ ¼çš„â€œå¤šæ ·æœ¬ force plotâ€ï¼ˆé™æ€ PNGï¼‰ã€‚\n",
    "    é‡è¦ï¼šsv å¿…é¡»ä¸ X_df çš„è¡Œåˆ—å®Œå…¨ä¸€è‡´ï¼ˆåŒæ ·æœ¬ã€åŒåˆ—é¡ºåºï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    # ----- 1) å– SHAP æ•°ç»„ï¼ˆæ­£ç±»ï¼‰ -----\n",
    "    sv_values = sv.values if hasattr(sv, \"values\") else sv\n",
    "    sv_values = np.asarray(sv_values)\n",
    "    if sv_values.ndim == 3:\n",
    "        sv_values = sv_values[..., 1]\n",
    "\n",
    "    # å®‰å…¨æ£€æŸ¥\n",
    "    n_samples, n_feats = sv_values.shape\n",
    "    feat_cols = np.array(X_df.columns.tolist())\n",
    "    assert n_feats == len(feat_cols), \\\n",
    "        f\"sv åˆ—æ•°({n_feats})ä¸ X_df åˆ—æ•°({len(feat_cols)})ä¸ä¸€è‡´ï¼›è¯·ä¿è¯ä¸ explainer è¾“å…¥ä¸€è‡´ã€‚\"\n",
    "    assert n_samples == len(X_df), \\\n",
    "        f\"sv è¡Œæ•°({n_samples})ä¸ X_df({len(X_df)})ä¸ä¸€è‡´ï¼›ä¸è¦åœ¨è§£é‡Šåå†é‡æ’/æŠ½æ ·ã€‚\"\n",
    "\n",
    "    # ----- 2) æ ·æœ¬æ’åº -----\n",
    "    if order_scores is not None:\n",
    "        order_scores = np.asarray(order_scores)\n",
    "        assert order_scores.shape[0] == n_samples, \"order_scores ä¸æ ·æœ¬æ•°ä¸ä¸€è‡´ã€‚\"\n",
    "        order = np.argsort(order_scores)[::-1]  # æ¦‚ç‡ä»é«˜åˆ°ä½\n",
    "    else:\n",
    "        order = np.argsort(np.sum(np.abs(sv_values), axis=1))[::-1]  # |SHAP| æ€»å’Œ\n",
    "    S = sv_values[order, :]\n",
    "\n",
    "    # ----- 3) é€‰å‰ top_n ç‰¹å¾ï¼ˆæˆ–ä½¿ç”¨å›ºå®šç´¢å¼•ï¼‰ -----\n",
    "    top_n = int(max(1, min(top_n, n_feats)))\n",
    "    if fixed_top_idx is None:\n",
    "        mean_abs = np.nanmean(np.abs(S), axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[-top_n:][::-1]\n",
    "    else:\n",
    "        top_idx = np.array(fixed_top_idx, dtype=int)\n",
    "        top_idx = top_idx[(top_idx >= 0) & (top_idx < n_feats)]\n",
    "        if len(top_idx) == 0:\n",
    "            raise ValueError(\"fixed_top_idx å…¨éƒ¨è¶Šç•Œã€‚\")\n",
    "    feat_names = feat_cols[top_idx].tolist()\n",
    "\n",
    "    # ----- 4) é…è‰²ï¼šé«˜é¥±å’Œå¯¹æ¯” -----\n",
    "    palette = [\n",
    "        \"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\",\n",
    "        \"#ffff33\", \"#a65628\", \"#f781bf\", \"#999999\", \"#66c2a5\",\n",
    "        \"#1b9e77\", \"#d95f02\", \"#7570b3\"  # ä»¥é˜² top_n > 10\n",
    "    ]\n",
    "    colors = [palette[i % len(palette)] for i in range(len(top_idx))]\n",
    "\n",
    "    # ----- 5) ç»˜å›¾ -----\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(S.shape[0])\n",
    "\n",
    "    # y è½´åˆ†ä½è£å‰ªèŒƒå›´é¢„å…ˆç®—å¥½ï¼ˆä¾¿äºä¸¤æ¬¡è°ƒç”¨ä¿æŒä¸€è‡´ï¼‰\n",
    "    if ylim is None and clip_pct is not None:\n",
    "        y_all = S[:, top_idx].ravel()\n",
    "        y_all = y_all[np.isfinite(y_all)]\n",
    "        if y_all.size > 20:\n",
    "            lo, hi = np.nanpercentile(y_all, list(clip_pct))\n",
    "            ylim = (float(lo), float(hi))\n",
    "\n",
    "    for k, (c, j, name) in enumerate(zip(colors, top_idx, feat_names)):\n",
    "        y = S[:, j].astype(float)\n",
    "        if ylim is not None:\n",
    "            y = np.clip(y, ylim[0], ylim[1])\n",
    "        if smooth and smooth > 1:\n",
    "            y = pd.Series(y).rolling(int(smooth), min_periods=1, center=True).mean().values\n",
    "        lw = 1.8 if k < max_legend else 1.2  # å¤šä½™çº¿æ¡ç•¥ç»†\n",
    "        plt.plot(x, y, color=c, lw=lw, alpha=0.95, label=name if k < max_legend else None)\n",
    "\n",
    "    plt.axhline(0, color=\"black\", lw=1.0)\n",
    "    plt.xlim(0, S.shape[0])\n",
    "    if ylim is not None:\n",
    "        # å†ç¨å¾®ç•™ç™½ï¼Œè§†è§‰ä¸è´´è¾¹\n",
    "        pad = 0.03 * (ylim[1] - ylim[0])\n",
    "        plt.ylim(ylim[0] - pad, ylim[1] + pad)\n",
    "\n",
    "    plt.xlabel(\"Pneumonia Patients\", fontsize=11)\n",
    "    plt.ylabel(\"SHAP value contribution\", fontsize=11)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=13, pad=10, fontweight=\"bold\")\n",
    "\n",
    "    # ----- 6) å›¾ä¾‹ï¼šæ§åˆ¶æ•°é‡ï¼Œé¿å…æº¢å‡º -----\n",
    "    if legend_out:\n",
    "        plt.legend(\n",
    "            title=\"Top features\",\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.03, 0.5),\n",
    "            frameon=False,\n",
    "            fontsize=9.5,\n",
    "            title_fontsize=10.5\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "    else:\n",
    "        plt.legend(frameon=False, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    return top_idx\n",
    "\n",
    "\n",
    "# â€”â€” é€‰æ‹©ç”¨äºç”Ÿæˆæ¦‚ç‡çš„â€œæœ€ç»ˆæ¨¡å‹â€ï¼šä¼˜å…ˆæ ¡å‡†åçš„ â€”â€” \n",
    "_final_est_for_plot = calibrated_best if ('calibrated_best' in globals()) else best_est\n",
    "\n",
    "# â€”â€” è®­ç»ƒé›†ï¼šæŒ‰æ¨¡å‹â€œæ¦‚ç‡â€æ’åºï¼Œè¿”å› top_idx â€”â€” \n",
    "train_scores = _proba_from_any(_final_est_for_plot, train_X_imp.loc[:, selected_all])\n",
    "top_idx = plot_force_like_lines(\n",
    "    sv_train,\n",
    "    train_X_imp.loc[:, selected_all],\n",
    "    title=f\"Force-like plot â€” MIMIC-IV (Train){' (Calibrated)' if 'calibrated_best' in globals() else ''}\",\n",
    "    top_n=10,\n",
    "    smooth=5,\n",
    "    order_scores=train_scores\n",
    ")\n",
    "\n",
    "# â€”â€” æµ‹è¯•é›†ï¼šç”¨è®­ç»ƒé˜¶æ®µçš„ top_idxï¼›ä¿æŒ y è½´ä¸€è‡´ä¾¿äºå¯¹æ¯” â€”â€” \n",
    "test_scores = _proba_from_any(_final_est_for_plot, test_X_imp.loc[:, selected_all])\n",
    "\n",
    "# ç»Ÿä¸€ y è½´èŒƒå›´ï¼šåŸºäº train/test ä¸¤è¾¹çš„ sv å…±åŒå†³å®šï¼ˆç¨³ï¼‰\n",
    "_sv_train_arr = sv_train.values if hasattr(sv_train, \"values\") else sv_train\n",
    "if _sv_train_arr.ndim == 3: _sv_train_arr = _sv_train_arr[..., 1]\n",
    "_sv_test_arr  = sv_test.values  if hasattr(sv_test, \"values\")  else sv_test\n",
    "if _sv_test_arr.ndim  == 3: _sv_test_arr  = _sv_test_arr[..., 1]\n",
    "\n",
    "y_abs_max = np.nanmax(np.abs(np.concatenate([_sv_train_arr[:, top_idx], _sv_test_arr[:, top_idx]], axis=0)))\n",
    "# ç»™ä¸€ç‚¹ä½™é‡\n",
    "ylim_shared = (-1.05 * y_abs_max, 1.05 * y_abs_max)\n",
    "\n",
    "_ = plot_force_like_lines(\n",
    "    sv_test,\n",
    "    test_X_imp.loc[:, selected_all],\n",
    "    title=f\"Force-like plot â€” MIMIC-IV (Test){' (Calibrated)' if 'calibrated_best' in globals() else ''}\",\n",
    "    smooth=5,\n",
    "    order_scores=test_scores,\n",
    "    fixed_top_idx=top_idx,\n",
    "    ylim=ylim_shared\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
