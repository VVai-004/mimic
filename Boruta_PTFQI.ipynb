{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb26c6f-4974-4da1-a77a-6609e22afe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. ç¯å¢ƒä¸ä¾èµ– ==========\n",
    "import os, json, warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import shap, math\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, recall_score, confusion_matrix, precision_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.utils import check_random_state\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logit, expit\n",
    "from numpy.random import default_rng\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "\n",
    "\n",
    "# ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªéšæœºç§å­\n",
    "RANDOM_STATE = 2025\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def setup_chinese_fonts():\n",
    "    \"\"\"\n",
    "    é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º\n",
    "    \"\"\"\n",
    "    # æ–¹æ¡ˆ1ï¼šä½¿ç”¨ç³»ç»Ÿä¸­æ–‡å­—ä½“ï¼ˆæ¨èï¼‰\n",
    "    try:\n",
    "        rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']\n",
    "        rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "        print(\"âœ… ä¸­æ–‡å­—ä½“é…ç½®æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ä¸­æ–‡å­—ä½“é…ç½®å¤±è´¥: {e}\")\n",
    "        print(\"   ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ\")\n",
    "\n",
    "# ç«‹å³é…ç½®\n",
    "setup_chinese_fonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0e552-b3ed-4bd9-9dd7-9b5c93ea0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. è¯»å–æ•°æ® ==========\n",
    "DATA_PATH = r\"C:/Users/VVai/Desktop/output/æ¸…æ´—/æ•´åˆæ•°æ®æ•´ç†/output_with_CCI_total.csv\"\n",
    "\n",
    "# âš ï¸ é‡è¦ï¼šæ˜ç¡®æ‚¨çš„ç›®æ ‡å˜é‡\n",
    "# é€‰é¡¹ 1ï¼šé¢„æµ‹ ICU å…¥ä½ â†’ TARGET = \"ICU_flag\"\n",
    "# é€‰é¡¹ 2ï¼šé¢„æµ‹æ­»äº¡     â†’ TARGET = \"death_flag\"\n",
    "TARGET = \"death_flag\"  # ğŸ”´ æ ¹æ®æ‚¨çš„ç ”ç©¶ç›®æ ‡ä¿®æ”¹è¿™é‡Œ\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"æ•°æ®å½¢çŠ¶:\", df.shape)\n",
    "print(\"åˆ—å(é¢„è§ˆ)å‰30ä¸ª:\", list(df.columns)[:30])\n",
    "assert TARGET in df.columns, f\"æœªæ‰¾åˆ°ç›®æ ‡å˜é‡åˆ— {TARGET}\"\n",
    "\n",
    "# å€™é€‰ç‰¹å¾(å­˜åœ¨å³ç”¨)\n",
    "candidate_cols = [\n",
    "    # åŸºæœ¬ä¿¡æ¯\n",
    "    \"age\", \"gender\",\n",
    "\n",
    "    # ä½“æ ¼ä¿¡æ¯\n",
    "    \"Height\", \"Height (Inches)\", \"Weight\", \"Weight (Lbs)\", \"BMI\", \"BMI (kg/m2)\",\n",
    "\n",
    "    # è¡€å‹\n",
    "    \"Diastolic blood pressure\", \"Systolic blood pressure\",\n",
    "\n",
    "    # ç”²åŠŸç›¸å…³\n",
    "    \"PTFQI\",\n",
    "\n",
    "    # è¡€æ¶²/ç”ŸåŒ–æŒ‡æ ‡\n",
    "    \"RBC(m/uL)\", \"WBC(K/uL)\", \"PLT\", \"Hb(g/dL)\", \"Glucose(mg/dL)\",\n",
    "    \"Creatinine(mg/dL)\", \"BUN(mg/dL)\", \"ALT(IU/L)\", \"AST\", \"PT\",\n",
    "\n",
    "    # ç–¾ç—…/å¹¶å‘ç—‡\n",
    "    \"Hypertension\", \"Diabetes\", \"CKD\", \"Myocardial infarct\",\n",
    "    \"Congestive heart failure\", \"Malignant cancer\", \"COPD\",\n",
    "\n",
    "    # ä½é™¢ä¿¡æ¯ï¼ˆæ ¹æ®ç›®æ ‡å˜é‡è°ƒæ•´ï¼‰\n",
    "    \"CCI Score\",\n",
    "]\n",
    "\n",
    "# âœ… å…³é”®æ­¥éª¤ï¼šä»ç‰¹å¾ä¸­æ’é™¤ç›®æ ‡å˜é‡\n",
    "feature_cols = [c for c in candidate_cols if c in df.columns and c != TARGET]\n",
    "\n",
    "# âœ… å®‰å…¨æ£€æŸ¥ 1ï¼šç¡®ä¿ç›®æ ‡å˜é‡ä¸åœ¨ç‰¹å¾ä¸­\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” å®‰å…¨æ£€æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç›®æ ‡å˜é‡: {TARGET}\")\n",
    "print(f\"å€™é€‰ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "\n",
    "if TARGET in feature_cols:\n",
    "    print(\"âŒ ä¸¥é‡é”™è¯¯ï¼šç›®æ ‡å˜é‡å‡ºç°åœ¨ç‰¹å¾åˆ—è¡¨ä¸­ï¼\")\n",
    "    print(\"   è¿™ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼Œç»“æœå®Œå…¨ä¸å¯ä¿¡ï¼\")\n",
    "    raise ValueError(f\"ç›®æ ‡å˜é‡ {TARGET} ä¸èƒ½ä½œä¸ºç‰¹å¾ï¼\")\n",
    "else:\n",
    "    print(f\"âœ… é€šè¿‡ï¼šç›®æ ‡å˜é‡ '{TARGET}' æœªå‡ºç°åœ¨ç‰¹å¾ä¸­\")\n",
    "\n",
    "# âœ… å®‰å…¨æ£€æŸ¥ 2ï¼šæ‰“å°ç‰¹å¾åˆ—è¡¨ä¾›äººå·¥å®¡æ ¸\n",
    "print(f\"\\nç”¨äºå»ºæ¨¡çš„ç‰¹å¾åˆ—è¡¨ ({len(feature_cols)} ä¸ª):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# æ€§åˆ«æ˜ å°„(å¦‚æ˜¯å­—ç¬¦ä¸²)\n",
    "if \"gender\" in feature_cols and df[\"gender\"].dtype == object:\n",
    "    df[\"gender\"] = df[\"gender\"].map({\"M\":1, \"F\":0}).fillna(0).astype(int)\n",
    "    print(\"\\nâœ… æ€§åˆ«å·²æ˜ å°„: M â†’ 1, F â†’ 0\")\n",
    "\n",
    "# æ„å»ºå»ºæ¨¡æ•°æ®é›†\n",
    "df_model = df[feature_cols + [TARGET]].dropna().copy()\n",
    "X_all = df_model[feature_cols]\n",
    "y_all = df_model[TARGET].astype(int)\n",
    "\n",
    "# âœ… æœ€ç»ˆéªŒè¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ ·æœ¬æ•°é‡: {X_all.shape[0]}\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_all.shape[1]}\")\n",
    "print(f\"ç›®æ ‡å˜é‡: {TARGET}\")\n",
    "print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "print(y_all.value_counts())\n",
    "print(f\"æ­£ç±»æ¯”ä¾‹: {y_all.mean():.2%}\")\n",
    "\n",
    "# âœ… æœ€åæ£€æŸ¥ï¼šç¡®è®¤ X_all ä¸­æ²¡æœ‰ç›®æ ‡å˜é‡\n",
    "if TARGET in X_all.columns:\n",
    "    raise ValueError(f\"âŒ è‡´å‘½é”™è¯¯ï¼šç›®æ ‡å˜é‡ {TARGET} åœ¨ç‰¹å¾çŸ©é˜µä¸­ï¼\")\n",
    "print(f\"\\nâœ… æœ€ç»ˆç¡®è®¤ï¼šç‰¹å¾çŸ©é˜µä¸­ä¸å«ç›®æ ‡å˜é‡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åœ¨ç¬¬2æ­¥æœ«å°¾æ·»åŠ \n",
    "print(f\"\\nğŸ“Š ç±»åˆ«åˆ†å¸ƒåˆ†æ:\")\n",
    "print(f\"  è´Ÿç±»(å­˜æ´»): {(y_all==0).sum()} ({(y_all==0).mean():.1%})\")\n",
    "print(f\"  æ­£ç±»(æ­»äº¡): {(y_all==1).sum()} ({(y_all==1).mean():.1%})\")\n",
    "print(f\"  ä¸å¹³è¡¡æ¯”: 1:{(y_all==0).sum()/(y_all==1).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬3æ­¥:æ•°æ®é›†åˆ’åˆ† ============================\n",
    "# âœ… ä¿®æ”¹: ç»Ÿä¸€ä½¿ç”¨ RANDOM_STATE\n",
    "\n",
    "# Step 1: å…ˆåˆ’åˆ†å‡ºæµ‹è¯•é›†(10%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,  # âœ… ä¿®æ”¹: ä½¿ç”¨ç»Ÿä¸€çš„éšæœºç§å­\n",
    "    stratify=y_all  # ä¿è¯ç±»åˆ«æ¯”ä¾‹ä¸€è‡´\n",
    ")\n",
    "\n",
    "# Step 2: å‰©ä¸‹çš„ 90% å†æŒ‰ 8:1 åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=1/9,   # 90% çš„ 1/9 â‰ˆ 10%\n",
    "    random_state=RANDOM_STATE,  # âœ… ä¿®æ”¹: ä½¿ç”¨ç»Ÿä¸€çš„éšæœºç§å­\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# ä¿å­˜å‰¯æœ¬(ç”¨äºå½’ä¸€åŒ– / ç‰¹å¾é€‰æ‹©)\n",
    "train_y = y_train.copy()\n",
    "val_y = y_val.copy()\n",
    "test_y = y_test.copy()\n",
    "\n",
    "\n",
    "print(\"æ•°æ®é›†åˆ’åˆ†å®Œæ¯•:\")\n",
    "print(f\"è®­ç»ƒé›†:{X_train.shape}, éªŒè¯é›†:{X_val.shape}, æµ‹è¯•é›†:{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬4æ­¥:æ•°æ®é¢„å¤„ç† ============================\n",
    "\n",
    "# âœ… ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æœ€ç»ˆå˜é‡åï¼Œé¿å…ä¸­é—´å˜é‡\n",
    "train_X_scaled = X_train.copy()\n",
    "val_X_scaled = X_val.copy()\n",
    "test_X_scaled = X_test.copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¼€å§‹æ•°æ®é¢„å¤„ç†\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è®­ç»ƒé›†: {train_X_scaled.shape}\")\n",
    "print(f\"éªŒè¯é›†: {val_X_scaled.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {test_X_scaled.shape}\")\n",
    "\n",
    "# è¯†åˆ«å˜é‡ç±»å‹\n",
    "binary_cols = []\n",
    "cont_cols = []\n",
    "\n",
    "for col in train_X_scaled.columns:\n",
    "    unique_vals = train_X_scaled[col].dropna().unique()\n",
    "    if set(unique_vals).issubset({0, 1}) or set(unique_vals).issubset({0.0, 1.0}):\n",
    "        binary_cols.append(col)\n",
    "    else:\n",
    "        cont_cols.append(col)\n",
    "\n",
    "print(f\"\\näºŒåˆ†ç±»å˜é‡ ({len(binary_cols)}ä¸ª): {binary_cols}\")\n",
    "print(f\"è¿ç»­å˜é‡ ({len(cont_cols)}ä¸ª): {cont_cols}\")\n",
    "\n",
    "# ç¼ºå¤±å€¼å¡«å……\n",
    "if len(cont_cols) > 0:\n",
    "    imputer_cont = SimpleImputer(strategy='median')\n",
    "    train_X_scaled[cont_cols] = imputer_cont.fit_transform(train_X_scaled[cont_cols])\n",
    "    val_X_scaled[cont_cols] = imputer_cont.transform(val_X_scaled[cont_cols])\n",
    "    test_X_scaled[cont_cols] = imputer_cont.transform(test_X_scaled[cont_cols])\n",
    "    print(f\"âœ… è¿ç»­å˜é‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ (ä¸­ä½æ•°)\")\n",
    "\n",
    "if len(binary_cols) > 0:\n",
    "    imputer_binary = SimpleImputer(strategy='most_frequent')\n",
    "    train_X_scaled[binary_cols] = imputer_binary.fit_transform(train_X_scaled[binary_cols])\n",
    "    val_X_scaled[binary_cols] = imputer_binary.transform(val_X_scaled[binary_cols])\n",
    "    test_X_scaled[binary_cols] = imputer_binary.transform(test_X_scaled[binary_cols])\n",
    "    print(f\"âœ… äºŒåˆ†ç±»å˜é‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ (ä¼—æ•°)\")\n",
    "\n",
    "# åæ–œå¤„ç†\n",
    "skew_threshold = 2.0\n",
    "highly_skewed = []\n",
    "for col in cont_cols:\n",
    "    skewness = train_X_scaled[col].skew()\n",
    "    if abs(skewness) > skew_threshold and train_X_scaled[col].min() >= 0:\n",
    "        highly_skewed.append(col)\n",
    "\n",
    "if len(highly_skewed) > 0:\n",
    "    print(f\"\\næ£€æµ‹åˆ° {len(highly_skewed)} ä¸ªé«˜åæ–œå˜é‡: {highly_skewed}\")\n",
    "    train_X_scaled[highly_skewed] = np.log1p(train_X_scaled[highly_skewed])\n",
    "    val_X_scaled[highly_skewed] = np.log1p(val_X_scaled[highly_skewed])\n",
    "    test_X_scaled[highly_skewed] = np.log1p(test_X_scaled[highly_skewed])\n",
    "    print(f\"âœ… å·²è¿›è¡Œ log1p è½¬æ¢\")\n",
    "\n",
    "# æ ‡å‡†åŒ– (ä»…å¯¹è¿ç»­å˜é‡)\n",
    "scaler = RobustScaler()\n",
    "train_X_scaled[cont_cols] = scaler.fit_transform(train_X_scaled[cont_cols])\n",
    "val_X_scaled[cont_cols] = scaler.transform(val_X_scaled[cont_cols])\n",
    "test_X_scaled[cont_cols] = scaler.transform(test_X_scaled[cont_cols])\n",
    "\n",
    "print(f\"\\nâœ… é¢„å¤„ç†å®Œæˆ\")\n",
    "print(f\"   - ç¼ºå¤±å€¼å¡«å……: âœ…\")\n",
    "print(f\"   - åæ–œå¤„ç†: {len(highly_skewed)} ä¸ªå˜é‡\")\n",
    "print(f\"   - æ ‡å‡†åŒ–: {len(cont_cols)} ä¸ªè¿ç»­å˜é‡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# âœ… æ·»åŠ åœ¨ç¬¬4æ­¥æœ«å°¾ï¼ˆ\"é¢„å¤„ç†å®Œæˆ\"ä¹‹åï¼‰\n",
    "# åˆ›å»ºå˜é‡åˆ«åï¼Œä¾›åç»­å¯è§†åŒ–ä½¿ç”¨\n",
    "train_X_imp = train_X_scaled.copy()\n",
    "val_X_imp = val_X_scaled.copy()\n",
    "test_X_imp = test_X_scaled.copy()\n",
    "\n",
    "print(f\"âœ… å·²åˆ›å»ºé¢„å¤„ç†åçš„æ•°æ®å‰¯æœ¬: train_X_imp, val_X_imp, test_X_imp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬5æ­¥:Boruta ç‰¹å¾é€‰æ‹© ============================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"å¼€å§‹ Boruta ç‰¹å¾é€‰æ‹©\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Boruta å‚æ•°è®¾ç½®\n",
    "selector = BorutaPy(\n",
    "    RandomForestClassifier(\n",
    "        n_jobs=-1, \n",
    "        class_weight='balanced', \n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    n_estimators='auto',\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "selector.fit(train_X_scaled.values, train_y.values)\n",
    "\n",
    "# è·å–é€‰ä¸­çš„ç‰¹å¾\n",
    "selected_features = train_X_scaled.columns[selector.support_].tolist()\n",
    "tentative_features = train_X_scaled.columns[selector.support_weak_].tolist()\n",
    "selected_all = selected_features + tentative_features\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"âœ… Boruta ç‰¹å¾é€‰æ‹©å®Œæˆ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ç¡®è®¤é€‰ä¸­çš„ç‰¹å¾ ({len(selected_features)}): {selected_features}\")\n",
    "print(f\"å¾…å®šç‰¹å¾ ({len(tentative_features)}): {tentative_features}\")\n",
    "print(f\"æœ€ç»ˆä½¿ç”¨çš„ç‰¹å¾æ€»æ•° ({len(selected_all)}): {selected_all}\")\n",
    "\n",
    "# ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': train_X_scaled.columns,\n",
    "    'Selected': selector.support_,\n",
    "    'Tentative': selector.support_weak_,\n",
    "    'Ranking': selector.ranking_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "print(\"\\nç‰¹å¾é‡è¦æ€§æ’å:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# âœ… å…³é”®æ­¥éª¤ï¼šç«‹å³åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å˜é‡å\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"åº”ç”¨ç‰¹å¾é€‰æ‹©åˆ°æ‰€æœ‰æ•°æ®é›†\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_X_sel = train_X_scaled[selected_all].copy()\n",
    "val_X_sel = val_X_scaled[selected_all].copy()\n",
    "test_X_sel = test_X_scaled[selected_all].copy()\n",
    "\n",
    "# âœ… ç»Ÿä¸€å˜é‡åï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨\n",
    "X_train_for_models = train_X_sel\n",
    "X_val_for_models = val_X_sel\n",
    "X_test_for_models = test_X_sel\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {train_X_sel.shape}\")\n",
    "print(f\"éªŒè¯é›†: {val_X_sel.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {test_X_sel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Boruta å†…éƒ¨æœºåˆ¶å¯è§†åŒ–ï¼ˆZ-score åˆ†å¸ƒ + å±±è„Šå›¾ï¼‰=====================\n",
    "# ---------- å¯é…ç½® ----------\n",
    "N_ITERS = 200\n",
    "SAMPLE_FRAC = 0.8\n",
    "SEED = 42\n",
    "\n",
    "# ---------- âœ… ä½¿ç”¨å»ºæ¨¡æ—¶ç›¸åŒçš„ç‰¹å¾é›† ----------\n",
    "# âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ selected_all è€Œä¸æ˜¯ selected_features\n",
    "X_sel = train_X_scaled[selected_all].copy()  # âœ… ä¿®æ”¹ï¼šä½¿ç”¨å®Œæ•´çš„ç‰¹å¾é›†\n",
    "y_sel = train_y.copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¨ Boruta å±±è„Šå›¾å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¯è§†åŒ–ç‰¹å¾æ•°: {len(selected_all)}\")\n",
    "print(f\"  - ç¡®è®¤ç‰¹å¾: {len(selected_features)}\")\n",
    "print(f\"  - å¾…å®šç‰¹å¾: {len(tentative_features)}\")\n",
    "print(f\"è¿­ä»£æ¬¡æ•°: {N_ITERS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------- RF å‚æ•° ----------\n",
    "if 'selector' in globals():\n",
    "    # å¤ç”¨ Boruta ä¸­çš„ RF å‚æ•°\n",
    "    RF_PARAMS = selector.estimator.get_params()\n",
    "    RF_PARAMS.pop('random_state', None)\n",
    "else:\n",
    "    RF_PARAMS = dict(\n",
    "        n_estimators=300,\n",
    "        n_jobs=-1,\n",
    "        max_depth=5,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "# ---------- ç”Ÿæˆ Boruta-like Z-score åˆ†å¸ƒ ----------\n",
    "def collect_boruta_like_z(X_df, y, rf_params, n_iters=200, sample_frac=0.8, seed=42):\n",
    "    rng = default_rng(seed)\n",
    "    cols = list(X_df.columns)\n",
    "    n = len(X_df)\n",
    "    take = max(1, int(n * float(sample_frac)))\n",
    "    rows = []\n",
    "\n",
    "    base_params = dict(rf_params)\n",
    "    base_params.pop(\"random_state\", None)\n",
    "\n",
    "    for i in tqdm(range(n_iters), desc=\"Collecting Boruta-like Z\"):\n",
    "        idx = rng.choice(n, take, replace=True)\n",
    "        Xb = X_df.iloc[idx, :]\n",
    "        yb = y.iloc[idx]\n",
    "\n",
    "        X_shadow = pd.DataFrame(index=Xb.index)\n",
    "        for c in cols:\n",
    "            shuf = Xb[c].to_numpy().copy()\n",
    "            rng.shuffle(shuf)\n",
    "            X_shadow[c + \"_shadow\"] = shuf\n",
    "\n",
    "        X_aug = pd.concat([Xb.reset_index(drop=True), X_shadow.reset_index(drop=True)], axis=1)\n",
    "        rf_local = RandomForestClassifier(**base_params, random_state=int(seed) + i)\n",
    "        rf_local.fit(X_aug, yb.values)\n",
    "\n",
    "        imps = pd.Series(rf_local.feature_importances_, index=X_aug.columns)\n",
    "        imp_real   = imps.loc[cols].values\n",
    "        imp_shadow = imps.loc[[c + \"_shadow\" for c in cols]].values\n",
    "\n",
    "        sh_mean = float(np.mean(imp_shadow))\n",
    "        sh_std  = float(np.std(imp_shadow))\n",
    "        if sh_std == 0 or np.isnan(sh_std):\n",
    "            sh_std = 1e-12\n",
    "        z = (imp_real - sh_mean) / sh_std\n",
    "\n",
    "        rows.extend([(f, float(zv), i) for f, zv in zip(cols, z)])\n",
    "\n",
    "    long_df = pd.DataFrame(rows, columns=[\"feature\", \"z\", \"iter\"])\n",
    "    return long_df\n",
    "\n",
    "long_borutaZ = collect_boruta_like_z(\n",
    "    X_df=X_sel, y=y_sel,\n",
    "    rf_params=RF_PARAMS,\n",
    "    n_iters=N_ITERS, sample_frac=SAMPLE_FRAC, seed=SEED\n",
    ")\n",
    "\n",
    "# ---------- åˆå¹¶ Boruta åˆ¤å®šç»“æœï¼ˆç”¨äºç€è‰²ï¼‰ ----------\n",
    "decision_map = {}\n",
    "if 'selector' in globals(): \n",
    "    # True=Confirmedï¼›support_weak_=Tentativeï¼›å…¶ä½™è§†ä¸º Rejected\n",
    "    confirmed_feats = set(train_X_scaled.columns[np.where(selector.support_)[0]])  \n",
    "    tentative_feats_set = set(train_X_scaled.columns[np.where(selector.support_weak_)[0]])\n",
    "    for f in X_sel.columns:\n",
    "        if f in confirmed_feats:\n",
    "            decision_map[f] = \"Confirmed\"\n",
    "        elif f in tentative_feats_set:  # âœ… æ”¹åé¿å…å’Œå‰é¢çš„ tentative_features å†²çª\n",
    "            decision_map[f] = \"Tentative\"\n",
    "        else:\n",
    "            decision_map[f] = \"Rejected\"\n",
    "else:\n",
    "    decision_map = {f: \"Tentative\" for f in X_sel.columns}\n",
    "\n",
    "long_borutaZ[\"decision\"] = long_borutaZ[\"feature\"].map(decision_map)\n",
    "\n",
    "# ---------- æŒ‰ä¸­ä½æ•° Z æ’åºï¼ˆç”»å›¾ä»ä¸Šåˆ°ä¸‹æ›´ç›´è§‚ï¼‰ ----------\n",
    "feat_order = (long_borutaZ.groupby(\"feature\")[\"z\"]\n",
    "              .median().sort_values(ascending=False).index.tolist())\n",
    "feats_for_plot = feat_order[::-1]\n",
    "\n",
    "# ---------- Ridge å±±è„Šå›¾å‡½æ•° ----------\n",
    "def ridgeline_plot_borutaZ(df_long, feats,\n",
    "                           x_label=\"Boruta-like Z-score (vs. shadow)\",\n",
    "                           title=\"Distribution of Feature Strength vs. Shadow (Boruta style)\"):\n",
    "    style_map = {\n",
    "        'Confirmed': {'lw': 1.8, 'alpha': 0.9},\n",
    "        'Tentative': {'lw': 1.2, 'alpha': 0.75},\n",
    "        'Rejected' : {'lw': 1.0, 'alpha': 0.55},\n",
    "    }\n",
    "    color_map = {\n",
    "        'Confirmed': '#1f77b4',  # è“\n",
    "        'Tentative': '#9467bd',  # ç´«\n",
    "        'Rejected' : '#7f7f7f',  # ç°\n",
    "    }\n",
    "\n",
    "    all_z = df_long['z'].dropna().values\n",
    "    if len(all_z):\n",
    "        x_min = float(np.percentile(all_z, 0.5) - 0.5)\n",
    "        x_max = float(np.percentile(all_z, 99.5) + 0.5)\n",
    "    else:\n",
    "        x_min, x_max = -1.0, 1.0\n",
    "    xs = np.linspace(x_min, x_max, 350)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8.5, max(4.0, 0.44*len(feats)+2)))\n",
    "    y_offsets = np.arange(len(feats))\n",
    "\n",
    "    for y0, f in zip(y_offsets, feats):\n",
    "        sub = df_long.loc[df_long['feature'] == f]\n",
    "        dat = sub['z'].dropna().values\n",
    "        main_dec = sub['decision'].mode().iat[0] if len(sub) and not sub['decision'].mode().empty else 'Tentative'\n",
    "        style = style_map.get(main_dec, style_map['Tentative'])\n",
    "        c = color_map.get(main_dec, color_map['Tentative'])\n",
    "\n",
    "        if len(dat) < 5 or np.allclose(dat, dat[0]):\n",
    "            xm = float(np.mean(dat)) if len(dat) else 0.0\n",
    "            ax.plot([xm, xm], [y0, y0+0.85], lw=style['lw'], alpha=style['alpha'], color=c)\n",
    "            continue\n",
    "\n",
    "        kde = gaussian_kde(dat)\n",
    "        ys = kde(xs)\n",
    "        ys = ys / (ys.max() + 1e-12) * 0.8  # å½’ä¸€åŒ–é«˜åº¦\n",
    "\n",
    "        ax.fill_between(xs, y0, y0 + ys, alpha=style['alpha'], color=c)\n",
    "        ax.plot(xs, y0 + ys, lw=style['lw'], color=c)\n",
    "        ax.plot(xs, [y0]*len(xs), lw=0.6, color='white')\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_yticks(y_offsets + 0.4)\n",
    "    ax.set_yticklabels(feats)\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.grid(True, axis='x', ls='--', alpha=0.35)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(-0.5, len(feats) - 0.1)\n",
    "\n",
    "    handles, labels = [], []\n",
    "    for k in ['Confirmed', 'Tentative', 'Rejected']:\n",
    "        if (df_long['decision'] == k).any():\n",
    "            handles.append(Line2D([0], [0], lw=style_map[k]['lw'], alpha=style_map[k]['alpha'], color=color_map[k]))\n",
    "            labels.append(k)\n",
    "    if handles:\n",
    "        ax.legend(handles, labels, title=\"Decision Type\", loc='center left',\n",
    "                  bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "    plt.show()\n",
    "\n",
    "# ---------- ç”»å›¾ ----------\n",
    "ridgeline_plot_borutaZ(\n",
    "    df_long=long_borutaZ,\n",
    "    feats=feats_for_plot,\n",
    "    x_label=\"Boruta-like Z-score (vs. shadow)\",\n",
    "    title=\"All Selected Features: Strength vs. Shadow\"\n",
    ")\n",
    "\n",
    "# ---------- å¯é€‰ï¼šå¯¼å‡ºæ±‡æ€»è¡¨ï¼Œä¾¿äºå†™æŠ¥å‘Š ----------\n",
    "borutaZ_summary = (long_borutaZ\n",
    "                   .groupby([\"feature\",\"decision\"])[\"z\"]\n",
    "                   .agg(['count','mean','median','std'])\n",
    "                   .reset_index()\n",
    "                   .sort_values(\"median\", ascending=False)).reset_index(drop=True)\n",
    "print(borutaZ_summary.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c5cf3-8a06-4db4-a181-7b14513cf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬6æ­¥ï¼šåŸºäº Boruta é€‰å‡ºçš„ç‰¹å¾ï¼Œå®šä¹‰æ¨¡å‹é›†åˆ =====================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å®šä¹‰æ¨¡å‹é›†åˆ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# âœ… éªŒè¯æ•°æ®å·²å‡†å¤‡å¥½ï¼ˆç¬¬5æ­¥å·²å®Œæˆï¼‰\n",
    "print(f\"ä½¿ç”¨ç‰¹å¾æ•°é‡: {X_train_for_models.shape[1]}\")\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {X_train_for_models.shape[0]}\")\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {X_val_for_models.shape[0]}\")\n",
    "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test_for_models.shape[0]}\")\n",
    "\n",
    "CLASS_WEIGHT = \"balanced\"\n",
    "\n",
    "# ---------- 6.1 CV ä¸è¯„åˆ†å™¨ ----------\n",
    "cv_5x3 = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE)\n",
    "auc_scorer = \"roc_auc\"\n",
    "\n",
    "# ---------- 6.2 å„æ¨¡å‹å®šä¹‰ ----------\n",
    "print(\"\\nå®šä¹‰æ¨¡å‹...\")\n",
    "\n",
    "# 1) Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    max_iter=5000, \n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "logit_grid = {\"C\": [0.0005, 0.001, 0.005]}\n",
    "logit_gs = GridSearchCV(logit, logit_grid, scoring=auc_scorer,\n",
    "                        cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                        error_score=\"raise\")\n",
    "\n",
    "# 2) LASSO / Elastic Net\n",
    "enet = LogisticRegression(\n",
    "    penalty=\"elasticnet\", \n",
    "    solver=\"saga\",\n",
    "    max_iter=5000,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "enet_grid = {\n",
    "    \"l1_ratio\": [0.7, 0.9, 1.0],\n",
    "    \"C\": [0.0005, 0.001, 0.005]\n",
    "}\n",
    "enet_gs = GridSearchCV(enet, enet_grid, scoring=auc_scorer,\n",
    "                       cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                       error_score=\"raise\")\n",
    "\n",
    "# 3) SVM (RBF)\n",
    "svm = SVC(\n",
    "    kernel=\"rbf\", \n",
    "    probability=True,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "svm_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "svm_gs = GridSearchCV(svm, svm_grid, scoring=auc_scorer,\n",
    "                      cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                      error_score=\"raise\")\n",
    "\n",
    "# 4) Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.5,\n",
    "    max_leaf_nodes=30,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    "    max_features=2,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf_grid = {\n",
    "    \"max_features\": [2, 3],\n",
    "    \"min_samples_leaf\": [10, 15, 20],\n",
    "    \"min_samples_split\": [10, 20]\n",
    "}\n",
    "rf_gs = GridSearchCV(rf, rf_grid, scoring=auc_scorer,\n",
    "                     cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                     error_score=\"raise\")\n",
    "\n",
    "# 5) XGBoost\n",
    "pos = int(np.sum(train_y == 1))\n",
    "neg = int(np.sum(train_y == 0))\n",
    "spw = (neg / max(pos, 1))\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=spw,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "xgb_grid = {\n",
    "    \"max_depth\": [2],\n",
    "    \"min_child_weight\": [80, 100, 150],   # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"gamma\": [5.0, 8.0, 10.0],            # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"reg_alpha\": [10.0, 15.0, 20.0],      # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"reg_lambda\": [10.0, 15.0, 20.0],     # è¿›ä¸€æ­¥å¢åŠ \n",
    "    \"colsample_bytree\": [0.2, 0.3],       # è¿›ä¸€æ­¥å‡å°‘\n",
    "    \"subsample\": [0.3, 0.4],              # è¿›ä¸€æ­¥å‡å°‘\n",
    "    \"learning_rate\": [0.005, 0.01],       # è¿›ä¸€æ­¥é™ä½\n",
    "    \"n_estimators\": [200, 300]\n",
    "}\n",
    "xgb_gs = GridSearchCV(xgb, xgb_grid, scoring=auc_scorer,\n",
    "                      cv=cv_5x3, n_jobs=-1, refit=True,\n",
    "                      error_score=\"raise\")\n",
    "\n",
    "# ---------- 6.3 æ¨¡å‹å­—å…¸ ----------\n",
    "models = {\n",
    "    \"Logistic\": logit_gs,\n",
    "    \"LASSO/ElasticNet\": enet_gs,\n",
    "    \"SVM\": svm_gs,\n",
    "    \"RandomForest\": rf_gs,\n",
    "    \"XGBoost\": xgb_gs\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… æ¨¡å‹å®šä¹‰å®Œæˆ\")\n",
    "print(f\"å°†å¯¹ä»¥ä¸‹ {len(models)} ä¸ªæ¨¡å‹è¿›è¡Œ 5Ã—3 CV è°ƒå‚ï¼š\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"   {i}. {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬6.5æ­¥ï¼šè¾…åŠ©å‡½æ•°å®šä¹‰ =====================\n",
    "# è¿™äº›å‡½æ•°å°†åœ¨ç¬¬7æ­¥çš„æ¨¡å‹è¯„ä¼°ä¸­ä½¿ç”¨\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å®šä¹‰è¯„ä¼°æ‰€éœ€çš„è¾…åŠ©å‡½æ•°\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def to_score(estimator, X):\n",
    "    \"\"\"\n",
    "    ä»ä»»æ„sklearnåˆ†ç±»å™¨è·å–æ­£ç±»æ¦‚ç‡åˆ†æ•°\n",
    "    å…¼å®¹Pipelineã€CalibratedClassifierCVç­‰\n",
    "    \"\"\"\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        proba = estimator.predict_proba(X)\n",
    "        if proba.ndim == 1:\n",
    "            return proba\n",
    "        elif proba.shape[1] == 2:\n",
    "            return proba[:, 1]  # äºŒåˆ†ç±»ï¼Œè¿”å›æ­£ç±»æ¦‚ç‡\n",
    "        else:\n",
    "            return proba[:, 1]  # å¤šåˆ†ç±»æ—¶ä¹Ÿè¿”å›ç¬¬äºŒåˆ—ï¼ˆå‡è®¾ç´¢å¼•1æ˜¯æ­£ç±»ï¼‰\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        return estimator.decision_function(X)\n",
    "    else:\n",
    "        return estimator.predict(X)\n",
    "\n",
    "\n",
    "def youden_threshold(y_true, y_score):\n",
    "    \"\"\"\n",
    "    æ ¹æ®YoudenæŒ‡æ•°ï¼ˆçµæ•åº¦+ç‰¹å¼‚åº¦-1çš„æœ€å¤§å€¼ï¼‰è®¡ç®—æœ€ä¼˜åˆ†ç±»é˜ˆå€¼\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, çœŸå®æ ‡ç­¾\n",
    "    y_score : array-like, é¢„æµ‹æ¦‚ç‡åˆ†æ•°\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    threshold : float, æœ€ä¼˜é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_index)\n",
    "    optimal_threshold = float(thresholds[optimal_idx])\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def binary_metrics_at(y_true, y_score, threshold):\n",
    "    \"\"\"\n",
    "    åœ¨ç»™å®šé˜ˆå€¼ä¸‹è®¡ç®—äºŒåˆ†ç±»çš„æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, çœŸå®æ ‡ç­¾ (0/1)\n",
    "    y_score : array-like, é¢„æµ‹æ¦‚ç‡åˆ†æ•°\n",
    "    threshold : float, åˆ†ç±»é˜ˆå€¼\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    accuracy, sensitivity, specificity, ppv, npv : float\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    \n",
    "    # æ ¹æ®é˜ˆå€¼è¿›è¡Œåˆ†ç±»\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    \n",
    "    # è®¡ç®—æ··æ·†çŸ©é˜µçš„å„ä¸ªç»„æˆéƒ¨åˆ†\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    # è®¡ç®—å„é¡¹æŒ‡æ ‡\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # çµæ•åº¦\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0  # ç‰¹å¼‚åº¦\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0          # é˜³æ€§é¢„æµ‹å€¼\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0          # é˜´æ€§é¢„æµ‹å€¼\n",
    "    \n",
    "    return float(accuracy), float(sensitivity), float(specificity), float(ppv), float(npv)\n",
    "\n",
    "\n",
    "def ece_quantile(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    è®¡ç®—Expected Calibration Error (ECE) - åˆ†ä½æ•°åˆ†ç®±ç‰ˆæœ¬\n",
    "    \n",
    "    ECEè¡¡é‡æ¨¡å‹çš„æ¦‚ç‡æ ¡å‡†è´¨é‡ï¼š\n",
    "    - ECEè¶Šæ¥è¿‘0ï¼Œè¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡è¶Šå‡†ç¡®\n",
    "    - ECE > 0.1 é€šå¸¸è®¤ä¸ºæ ¡å‡†è¾ƒå·®\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like, çœŸå®æ ‡ç­¾ (0/1)\n",
    "    y_prob : array-like, é¢„æµ‹æ¦‚ç‡\n",
    "    n_bins : int, åˆ†ç®±æ•°é‡ï¼ˆé»˜è®¤10ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ece : float, æœŸæœ›æ ¡å‡†è¯¯å·®\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "    \n",
    "    # ä½¿ç”¨åˆ†ä½æ•°åˆ†ç®±ï¼ˆæ›´ç¨³å¥ï¼‰\n",
    "    try:\n",
    "        bin_edges = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "        bin_edges[0] = 0.0\n",
    "        bin_edges[-1] = 1.0\n",
    "    except:\n",
    "        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    \n",
    "    ece = 0.0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        # æ‰¾åˆ°è½åœ¨å½“å‰binä¸­çš„æ ·æœ¬\n",
    "        bin_mask = (y_prob > bin_edges[i]) & (y_prob <= bin_edges[i + 1])\n",
    "        \n",
    "        if i == 0:  # ç¬¬ä¸€ä¸ªbinåŒ…å«å·¦è¾¹ç•Œ\n",
    "            bin_mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i + 1])\n",
    "        \n",
    "        bin_size = np.sum(bin_mask)\n",
    "        \n",
    "        if bin_size > 0:\n",
    "            bin_prob_mean = np.mean(y_prob[bin_mask])\n",
    "            bin_true_mean = np.mean(y_true[bin_mask])\n",
    "            ece += (bin_size / total_samples) * abs(bin_prob_mean - bin_true_mean)\n",
    "    \n",
    "    return float(ece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_ci_bootstrap(y_true, y_score, n_boot=1000, seed=42, alpha=0.95):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Bootstrap é‡é‡‡æ ·æ–¹æ³•è®¡ç®— AUC çš„ç½®ä¿¡åŒºé—´ï¼ˆè¶…é²æ£’ç‰ˆæœ¬ï¼‰\n",
    "    èƒ½å¤Ÿå¤„ç†é‡‡æ ·å¤±è´¥ã€ç©ºæ•°ç»„ç­‰å„ç§å¼‚å¸¸æƒ…å†µ\n",
    "    \"\"\"\n",
    "    # === ç¬¬0æ­¥ï¼šç±»å‹è½¬æ¢å’Œå®‰å…¨æ£€æŸ¥ ===\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    \n",
    "    # æ£€æŸ¥é•¿åº¦åŒ¹é…\n",
    "    if len(y_true) != len(y_score):\n",
    "        print(f\"âŒ é”™è¯¯: y_true å’Œ y_score é•¿åº¦ä¸åŒ¹é… ({len(y_true)} vs {len(y_score)})\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰ NaN æˆ– Inf\n",
    "    if np.any(np.isnan(y_score)) or np.any(np.isinf(y_score)):\n",
    "        print(f\"âŒ é”™è¯¯: y_score åŒ…å« NaN æˆ– Inf\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # === ç¬¬1æ­¥ï¼šæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ ===\n",
    "    if n < 10:\n",
    "        print(f\"âš ï¸ æ ·æœ¬æ•°å¤ªå°‘ (n={n})ï¼Œä½¿ç”¨ç‚¹ä¼°è®¡\")\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            return float(auc), float(auc)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è®¡ç®—AUCå¤±è´¥: {e}\")\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ\n",
    "    unique_classes = np.unique(y_true)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"âŒ åªæœ‰å•ä¸€ç±»åˆ«: {unique_classes}ï¼Œæ— æ³•è®¡ç®—AUC\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    \n",
    "    if n_pos < 2 or n_neg < 2:\n",
    "        print(f\"âš ï¸ æŸä¸ªç±»åˆ«æ ·æœ¬å¤ªå°‘ (æ­£ç±»:{n_pos}, è´Ÿç±»:{n_neg})ï¼Œä½¿ç”¨ç‚¹ä¼°è®¡\")\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            return float(auc), float(auc)\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # === ç¬¬2æ­¥ï¼šæ‰§è¡Œåˆ†å±‚ Bootstrap ===\n",
    "    aucs = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    # è·å–æ­£è´Ÿç±»ç´¢å¼•\n",
    "    pos_idx = np.where(y_true == 1)[0]\n",
    "    neg_idx = np.where(y_true == 0)[0]\n",
    "    \n",
    "    for i in range(n_boot):\n",
    "        try:\n",
    "            # åˆ†å±‚é‡‡æ ·ï¼šä»æ­£è´Ÿç±»ä¸­åˆ†åˆ«é‡‡æ ·ï¼Œç¡®ä¿æ¯æ¬¡éƒ½æœ‰ä¸¤ä¸ªç±»åˆ«\n",
    "            boot_pos_idx = rng.choice(pos_idx, size=len(pos_idx), replace=True)\n",
    "            boot_neg_idx = rng.choice(neg_idx, size=len(neg_idx), replace=True)\n",
    "            \n",
    "            # åˆå¹¶ç´¢å¼•\n",
    "            boot_idx = np.concatenate([boot_pos_idx, boot_neg_idx])\n",
    "            \n",
    "            # è®¡ç®— AUC\n",
    "            auc = roc_auc_score(y_true[boot_idx], y_score[boot_idx])\n",
    "            \n",
    "            # æ£€æŸ¥ AUC æ˜¯å¦æœ‰æ•ˆ\n",
    "            if np.isnan(auc) or np.isinf(auc):\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            aucs.append(float(auc))\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    # === ç¬¬3æ­¥ï¼šç»“æœéªŒè¯å’Œå¤„ç† ===\n",
    "    success_count = len(aucs)\n",
    "    success_rate = success_count / n_boot if n_boot > 0 else 0\n",
    "    \n",
    "    # å¦‚æœå®Œå…¨å¤±è´¥\n",
    "    if success_count == 0:\n",
    "        print(f\"âŒ æ‰€æœ‰ {n_boot} æ¬¡ Bootstrap é‡‡æ ·éƒ½å¤±è´¥\")\n",
    "        print(f\"   æ•°æ®ä¿¡æ¯: n={n}, æ­£ç±»={n_pos}, è´Ÿç±»={n_neg}\")\n",
    "        print(f\"   y_score èŒƒå›´: [{np.min(y_score):.4f}, {np.max(y_score):.4f}]\")\n",
    "        \n",
    "        # å°è¯•ç›´æ¥è®¡ç®—å…¨æ•°æ® AUC\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            print(f\"   âœ… ä½¿ç”¨å…¨æ•°æ®AUCä½œä¸ºæ›¿ä»£: {auc:.4f}\")\n",
    "            return float(auc), float(auc)\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ å…¨æ•°æ®AUCè®¡ç®—ä¹Ÿå¤±è´¥: {e}\")\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # å¦‚æœæˆåŠŸç‡è¿‡ä½ï¼ˆä½†ä¸æ˜¯0ï¼‰\n",
    "    if success_rate < 0.5:\n",
    "        print(f\"âš ï¸ Bootstrap æˆåŠŸç‡è¾ƒä½: {success_rate:.1%} ({success_count}/{n_boot})\")\n",
    "    \n",
    "    # === ç¬¬4æ­¥ï¼šè®¡ç®—ç½®ä¿¡åŒºé—´ ===\n",
    "    aucs = np.array(aucs, dtype=float)\n",
    "    \n",
    "    # å†æ¬¡æ£€æŸ¥æ•°ç»„æ˜¯å¦æœ‰æ•ˆï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰\n",
    "    if len(aucs) == 0:\n",
    "        print(\"âŒ aucs æ•°ç»„ä¸ºç©ºï¼ˆä¸åº”è¯¥åˆ°è¿™é‡Œï¼‰\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # è®¡ç®—ç™¾åˆ†ä½æ•°\n",
    "    lower_percentile = ((1.0 - alpha) / 2.0) * 100\n",
    "    upper_percentile = (alpha + (1.0 - alpha) / 2.0) * 100\n",
    "    \n",
    "    try:\n",
    "        ci_lower = float(np.percentile(aucs, lower_percentile))\n",
    "        ci_upper = float(np.percentile(aucs, upper_percentile))\n",
    "        return ci_lower, ci_upper\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®¡ç®—ç™¾åˆ†ä½æ•°å¤±è´¥: {e}\")\n",
    "        print(f\"   aucs æ•°ç»„ä¿¡æ¯: len={len(aucs)}, èŒƒå›´=[{np.min(aucs):.4f}, {np.max(aucs):.4f}]\")\n",
    "        # è¿”å›å‡å€¼ä½œä¸ºæ›¿ä»£\n",
    "        auc_mean = float(np.mean(aucs))\n",
    "        return auc_mean, auc_mean\n",
    "\n",
    "\n",
    "print(\"âœ… å·²æ›´æ–° auc_ci_bootstrap å‡½æ•°ï¼ˆè¶…é²æ£’ç‰ˆæœ¬ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ ç¬¬7æ­¥ï¼šè®­ç»ƒä¸è¯„ä¼° ============================\n",
    "\n",
    "# ===================== 7.1 CVè°ƒå‚ =====================\n",
    "fit_models = {}\n",
    "cv_report = {}\n",
    "grid_search_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"é˜¶æ®µ1ï¼šCVè°ƒå‚\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, est in models.items():\n",
    "    try:\n",
    "        est_ = clone(est)\n",
    "        est_.fit(X_train_for_models, train_y)\n",
    "        \n",
    "        best_est = est_.best_estimator_ if hasattr(est_, \"best_estimator_\") else est_\n",
    "        best_score = est_.best_score_ if hasattr(est_, \"best_score_\") else np.nan\n",
    "        \n",
    "        fit_models[name] = best_est\n",
    "        grid_search_results[name] = est_\n",
    "        cv_report[name] = best_score\n",
    "        \n",
    "        print(f\"[âœ“] {name:20s} | CV AUC = {best_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âœ—] {name:20s} | å¤±è´¥: {e}\")\n",
    "        fit_models[name] = None\n",
    "        grid_search_results[name] = None\n",
    "        cv_report[name] = np.nan\n",
    "\n",
    "\n",
    "# ===================== 7.2 éªŒè¯é›†é€‰æ‹©æœ€ä¼˜æ¨¡å‹ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ2ï¼šéªŒè¯é›†é€‰æ‹©æœ€ä¼˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "val_aucs = {}\n",
    "for name, est in fit_models.items():\n",
    "    if est is None:\n",
    "        val_aucs[name] = np.nan\n",
    "        continue\n",
    "    try:\n",
    "        y_val_prob = to_score(est, X_val_for_models)\n",
    "        val_aucs[name] = roc_auc_score(val_y, y_val_prob)\n",
    "        print(f\"[âœ“] {name:20s} | Val AUC = {val_aucs[name]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âœ—] {name:20s} | å¤±è´¥: {e}\")\n",
    "        val_aucs[name] = np.nan\n",
    "\n",
    "best_model_name = max(val_aucs.items(), key=lambda x: x[1])[0]\n",
    "best_model_params = fit_models[best_model_name]\n",
    "print(f\"\\nğŸ† æœ€ä¼˜æ¨¡å‹: {best_model_name} (Val AUC = {val_aucs[best_model_name]:.4f})\")\n",
    "\n",
    "\n",
    "# ===================== 7.3 ç¡®å®šæœ€ä¼˜æ¨¡å‹ï¼ˆä¸é‡è®­ç»ƒï¼‰ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ3ï¼šç¡®å®šæœ€ä¼˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æœ€ä¼˜æ¨¡å‹ç›´æ¥ä½¿ç”¨ 7.1 è®­ç»ƒå¥½çš„\n",
    "best_model = fit_models[best_model_name]\n",
    "\n",
    "# æ ¡å‡†ï¼ˆåªç”¨è®­ç»ƒé›†ï¼‰\n",
    "calibrated_best = CalibratedClassifierCV(estimator=clone(best_model), method=\"sigmoid\", cv=5)\n",
    "calibrated_best.fit(X_train_for_models, train_y)\n",
    "\n",
    "# é˜ˆå€¼ï¼ˆåªç”¨è®­ç»ƒé›†ï¼‰\n",
    "y_train_prob = to_score(calibrated_best, X_train_for_models)\n",
    "optimal_threshold = youden_threshold(train_y, y_train_prob)\n",
    "\n",
    "print(f\"âœ… æœ€ä¼˜æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"âœ… Youdené˜ˆå€¼ = {optimal_threshold:.4f}\")\n",
    "\n",
    "# ===================== 7.4 æ‰€æœ‰æ¨¡å‹è¯„ä¼°ï¼ˆTrain/Val/Test ç‹¬ç«‹ï¼‰ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ6ï¼šæ‰€æœ‰æ¨¡å‹è¯„ä¼°\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for name, model in fit_models.items():\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        calibrated = CalibratedClassifierCV(estimator=clone(model), method=\"sigmoid\", cv=5)\n",
    "        calibrated.fit(X_train_for_models, train_y)\n",
    "    except:\n",
    "        calibrated = model\n",
    "    \n",
    "    try:\n",
    "        train_prob_for_thr = to_score(calibrated, X_train_for_models)\n",
    "        threshold = youden_threshold(train_y, train_prob_for_thr)\n",
    "    except:\n",
    "        threshold = 0.5\n",
    "    \n",
    "    datasets = [\n",
    "        (\"Train\", X_train_for_models, train_y),\n",
    "        (\"Val\", X_val_for_models, val_y),\n",
    "        (\"Test\", X_test_for_models, test_y)\n",
    "    ]\n",
    "    \n",
    "    for split_name, Xs, yss in datasets:\n",
    "        try:\n",
    "            prob = to_score(calibrated, Xs)\n",
    "            auc_val = roc_auc_score(yss, prob)\n",
    "            ci_lo, ci_hi = auc_ci_bootstrap(yss, prob, n_boot=800, seed=RANDOM_STATE)\n",
    "            acc, sen, spe, ppv, npv = binary_metrics_at(yss, prob, threshold)\n",
    "            brier = brier_score_loss(yss, prob)\n",
    "            ece = ece_quantile(yss, prob)\n",
    "            \n",
    "            all_results.append({\n",
    "                \"Model\": name, \"Dataset\": split_name, \"AUC\": auc_val,\n",
    "                \"AUC 95%CI\": f\"[{ci_lo:.3f}, {ci_hi:.3f}]\", \"Threshold\": threshold,\n",
    "                \"Accuracy\": acc, \"Sensitivity\": sen, \"Specificity\": spe,\n",
    "                \"PPV\": ppv, \"NPV\": npv, \"Brier Score\": brier, \"ECE\": ece\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[âœ—] {name} @ {split_name}: {e}\")\n",
    "\n",
    "df_all_results = pd.DataFrame(all_results)\n",
    "\n",
    "# AUC å®½æ ¼å¼å¯¹æ¯”\n",
    "pivot_auc = df_all_results.pivot(index='Model', columns='Dataset', values='AUC')\n",
    "pivot_auc = pivot_auc[['Train', 'Val', 'Test']]\n",
    "pivot_auc['Train-Test Gap'] = pivot_auc['Train'] - pivot_auc['Test']\n",
    "pivot_auc = pivot_auc.sort_values('Test', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š AUC å¯¹æ¯”:\")\n",
    "display(pivot_auc.round(4))\n",
    "\n",
    "# æ‰€æœ‰æ¨¡å‹ä¸‰é›†è¯¦ç»†æ€§èƒ½\n",
    "print(\"\\nğŸ“Š æ‰€æœ‰æ¨¡å‹å®Œæ•´æ€§èƒ½è¯¦æƒ…:\")\n",
    "df_display = df_all_results.copy()\n",
    "df_display = df_display.sort_values(['Model', 'Dataset'], \n",
    "                                     key=lambda x: x.map({'Train': 0, 'Val': 1, 'Test': 2}) if x.name == 'Dataset' else x)\n",
    "df_display = df_display[['Model', 'Dataset', 'AUC', 'AUC 95%CI', 'Threshold', \n",
    "                          'Sensitivity', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'Brier Score', 'ECE']]\n",
    "display(df_display.round(4))\n",
    "\n",
    "# æœ€ä¼˜æ¨¡å‹\n",
    "df_test = df_all_results[df_all_results['Dataset'] == 'Test'].sort_values('AUC', ascending=False)\n",
    "best_model_name = df_test.iloc[0]['Model']\n",
    "print(f\"\\nğŸ† æœ€ä¼˜æ¨¡å‹: {best_model_name} (Test AUC = {df_test.iloc[0]['AUC']:.4f})\")\n",
    "\n",
    "\n",
    "# ===================== 7.5 æ‹Ÿåˆç¨‹åº¦å¯è§†åŒ– =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ7ï¼šæ‹Ÿåˆç¨‹åº¦å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # æ”¹ä¸º2åˆ—\n",
    "\n",
    "# --- å›¾1: åˆ†ç»„æŸ±çŠ¶å›¾ ---\n",
    "ax1 = axes[0]\n",
    "models_order = pivot_auc.index.tolist()\n",
    "x = np.arange(len(models_order))\n",
    "width = 0.25\n",
    "\n",
    "train_vals = [pivot_auc.loc[m, 'Train'] for m in models_order]\n",
    "val_vals = [pivot_auc.loc[m, 'Val'] for m in models_order]\n",
    "test_vals = [pivot_auc.loc[m, 'Test'] for m in models_order]\n",
    "\n",
    "bars1 = ax1.bar(x - width, train_vals, width, label='Train', color='#3498db', alpha=0.8)\n",
    "bars2 = ax1.bar(x, val_vals, width, label='Val', color='#2ecc71', alpha=0.8)\n",
    "bars3 = ax1.bar(x + width, test_vals, width, label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AUC', fontsize=12)\n",
    "ax1.set_title('Train / Val / Test AUC Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_order, rotation=30, ha='right')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "ax1.axhline(y=0.7, color='gray', linestyle='--', alpha=0.5, label='AUC=0.7')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# --- å›¾2: Train-Test Gap æŸ±çŠ¶å›¾ ---\n",
    "ax2 = axes[1]\n",
    "gaps = [pivot_auc.loc[m, 'Train-Test Gap'] for m in models_order]\n",
    "colors = ['#27ae60' if g < 0.05 else '#f39c12' if g < 0.10 else '#e67e22' if g < 0.15 else '#c0392b' for g in gaps]\n",
    "\n",
    "bars_gap = ax2.bar(x, gaps, width=0.5, color=colors, alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax2.set_ylabel('Train - Test Gap', fontsize=12)\n",
    "ax2.set_title('Overfitting Assessment (Train-Test Gap)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_order, rotation=30, ha='right')\n",
    "ax2.axhline(y=0.05, color='green', linestyle='--', alpha=0.7, linewidth=1.5, label='Good (<0.05)')\n",
    "ax2.axhline(y=0.10, color='orange', linestyle='--', alpha=0.7, linewidth=1.5, label='Acceptable (<0.10)')\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', alpha=0.7, linewidth=1.5, label='Concern (>0.15)')\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.set_ylim(0, max(gaps) * 1.3)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, gap in zip(bars_gap, gaps):\n",
    "    ax2.annotate(f'{gap:.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… å¯è§†åŒ–å·²ä¿å­˜: overfitting_analysis.png\")\n",
    "\n",
    "# ===================== 7.6 åˆ›å»ºåç»­å˜é‡ & ä¿å­˜æ¨¡å‹ =====================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é˜¶æ®µ8ï¼šä¿å­˜æ¨¡å‹\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ä½¿ç”¨ 7.3 ä¸­åˆ›å»ºçš„æ ¡å‡†æ¨¡å‹ï¼ˆä¸å†å¼•ç”¨ calibrated_finalï¼‰\n",
    "# calibrated_best å·²åœ¨ 7.3 ä¸­å®šä¹‰\n",
    "best_est = fit_models[best_model_name]\n",
    "calibrated_models = {best_model_name: calibrated_best}\n",
    "y_test_prob = to_score(calibrated_best, X_test_for_models)\n",
    "thr_star = optimal_threshold\n",
    "has_val = True  # ä½ æœ‰éªŒè¯é›†\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "import pickle\n",
    "test_auc_value = df_test[df_test['Model']==best_model_name]['AUC'].values[0]\n",
    "\n",
    "# è·å–æœ€ä¼˜å‚æ•°\n",
    "if grid_search_results.get(best_model_name) is not None:\n",
    "    best_params = grid_search_results[best_model_name].best_params_\n",
    "else:\n",
    "    best_params = None\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': calibrated_best,\n",
    "    'model_name': best_model_name,\n",
    "    'features': list(X_train_for_models.columns),\n",
    "    'threshold': optimal_threshold,\n",
    "    'best_params': best_params,\n",
    "    'metadata': {'train_size': len(X_train_for_models), 'test_auc': test_auc_value, 'random_state': RANDOM_STATE}\n",
    "}\n",
    "\n",
    "with open(\"best_model_complete.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜: best_model_complete.pkl\")\n",
    "print(f\"   æœ€ä¼˜æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"   é˜ˆå€¼: {optimal_threshold:.4f} | æµ‹è¯•AUC: {test_auc_value:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e88d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 7.7 ROC æ›²çº¿ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰ =====================\n",
    "\n",
    "# é¦–å…ˆæ”¶é›†æ‰€æœ‰æ ¡å‡†åçš„æ¨¡å‹ï¼ˆåœ¨ 7.4 è¯„ä¼°æ—¶å·²ç»åˆ›å»ºè¿‡ï¼Œè¿™é‡Œé‡æ–°æ”¶é›†ï¼‰\n",
    "calibrated_models_all = {}\n",
    "for name, model in fit_models.items():\n",
    "    if model is None:\n",
    "        continue\n",
    "    try:\n",
    "        calibrated = CalibratedClassifierCV(estimator=clone(model), method=\"sigmoid\", cv=5)\n",
    "        calibrated.fit(X_train_for_models, train_y)\n",
    "        calibrated_models_all[name] = calibrated\n",
    "    except:\n",
    "        calibrated_models_all[name] = model\n",
    "\n",
    "def plot_roc_panel(models_dict):\n",
    "    # å±•ç¤º Train, Val, Test ä¸‰ä¸ªæ•°æ®é›†\n",
    "    sets = [\n",
    "        (\"Train\", X_train_for_models, train_y),\n",
    "        (\"Val\", X_val_for_models, val_y),\n",
    "        (\"Test\", X_test_for_models, test_y)\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for ax, (set_name, Xs, ys) in zip(axes, sets):\n",
    "        for name, est in models_dict.items():\n",
    "            y_prob = to_score(est, Xs)\n",
    "            fpr, tpr, _ = roc_curve(ys, y_prob)\n",
    "            auc_val = roc_auc_score(ys, y_prob)\n",
    "            ax.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        ax.set_title(f\"ROC â€” {set_name}\", fontsize=13)\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend(loc=\"lower right\", fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    fig.suptitle(\"ROC Curves by Dataset (All Calibrated Models)\", y=1.02, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curves_all_models.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_panel(calibrated_models_all)\n",
    "\n",
    "\n",
    "# ===================== 7.8 æœ€ä¼˜æ¨¡å‹ ROCï¼ˆæ ‡å‡º Youden ç‚¹ï¼‰ =====================\n",
    "def plot_best_roc(y_true, y_prob, model_name, threshold):\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    idx = np.argmin(np.abs(thr - threshold))\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{model_name} (AUC={roc_auc_score(y_true, y_prob):.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.scatter(fpr[idx], tpr[idx], s=70, marker='o', color='red',\n",
    "                label=f\"Youden Threshold = {thr[idx]:.3f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Best Model ROC on Test â€” {model_name}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_best_model.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_best_roc(test_y, y_test_prob, best_model_name, thr_star)\n",
    "\n",
    "\n",
    "# ===================== 7.9 DCAï¼ˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ï¼‰ =====================\n",
    "def decision_curve(y_true, y_prob, thresholds=None):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(y_prob).astype(float)\n",
    "    n = len(y)\n",
    "    prev = y.mean()\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.60, 30)\n",
    "    out = []\n",
    "    for pt in thresholds:\n",
    "        pred = (p >= pt).astype(int)\n",
    "        tp = np.sum((pred == 1) & (y == 1))\n",
    "        fp = np.sum((pred == 1) & (y == 0))\n",
    "        nb_model = (tp / n) - (fp / n) * (pt / (1 - pt))\n",
    "        nb_all = prev - (1 - prev) * (pt / (1 - pt))\n",
    "        nb_none = 0.0\n",
    "        out.append((pt, nb_model, nb_all, nb_none))\n",
    "    return pd.DataFrame(out, columns=[\"pt\", \"NB_model\", \"NB_all\", \"NB_none\"])\n",
    "\n",
    "def plot_dca(df, title=\"\", mark_pt=None, db_name=\"MIMIC-IV\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(df[\"pt\"], df[\"NB_model\"], lw=2, label=\"Model\")\n",
    "    ax.plot(df[\"pt\"], df[\"NB_all\"], lw=2, color=\"gray\", label=\"Treat All\")\n",
    "    ax.plot(df[\"pt\"], df[\"NB_none\"], lw=2, ls=\"--\", color=\"gray\", label=\"Treat None\")\n",
    "\n",
    "    if mark_pt is not None:\n",
    "        row = df.iloc[(df[\"pt\"] - mark_pt).abs().argmin()]\n",
    "        ax.scatter([row[\"pt\"]], [row[\"NB_model\"]], s=45, label=f\"thrâ‰ˆ{row['pt']:.2f}\", color=\"red\")\n",
    "\n",
    "    ax.set_xlabel(\"Threshold Probability\")\n",
    "    ax.set_ylabel(\"Net Benefit\")\n",
    "    ax.set_title(f\"{title} â€” {db_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # âœ… æ·»åŠ è¿™ä¸€è¡Œï¼šå›ºå®š y è½´èŒƒå›´\n",
    "    ax.set_ylim(-0.05, 0.30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒé›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äº DCAï¼‰\n",
    "y_train_prob = to_score(calibrated_best, X_train_for_models)\n",
    "\n",
    "# è®­ç»ƒé›† DCA\n",
    "plot_dca(decision_curve(train_y, y_train_prob),\n",
    "         title=f\"DCA â€” Calibrated {best_model_name}\", mark_pt=thr_star, db_name=\"Train\")\n",
    "\n",
    "# æµ‹è¯•é›† DCA\n",
    "plot_dca(decision_curve(test_y, y_test_prob),\n",
    "         title=f\"DCA â€” Calibrated {best_model_name}\", mark_pt=thr_star, db_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddca48-01be-4053-abef-7bab6551a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== ç¬¬8æ­¥ï¼šSHAP è§£é‡Šåˆ†æ =====================\n",
    "print(\"=\" * 70)\n",
    "print(\"ç¬¬8æ­¥ï¼šSHAP è§£é‡Šåˆ†æï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------------- 8.1 å‡†å¤‡å·¥ä½œï¼šä½¿ç”¨Borutaç­›é€‰åçš„ç‰¹å¾ ----------------\n",
    "features_for_shap = list(X_train_for_models.columns)\n",
    "print(f\"\\nâœ… Borutaç­›é€‰ç‰¹å¾æ•°: {len(features_for_shap)}\")\n",
    "print(f\"   - ç¡®è®¤ç‰¹å¾: {len(selected_features)}\")\n",
    "print(f\"   - å¾…å®šç‰¹å¾: {len(tentative_features)}\")\n",
    "\n",
    "# ä½¿ç”¨æµ‹è¯•é›†è¿›è¡ŒSHAPè§£é‡Š\n",
    "X_shap = X_test_for_models.copy()\n",
    "y_shap = test_y.copy()\n",
    "dataset_name = \"Test\"\n",
    "\n",
    "print(f\"\\nâœ… ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œ SHAP è§£é‡Š (n={len(X_shap)})\")\n",
    "print(f\"   ç ”ç©¶ä¸»é¢˜: ç”²çŠ¶è…ºæ¿€ç´ æ•æ„ŸæŒ‡æ•°ä¸é‡ç—‡æ‚£è€…å…¨å› æ­»äº¡ç‡\")\n",
    "\n",
    "# è·å–æœ€ä¼˜æ¨¡å‹ï¼ˆå·²æ ¡å‡†ï¼‰\n",
    "shap_model = calibrated_best\n",
    "print(f\"\\nâœ… SHAP è§£é‡Šæ¨¡å‹: {best_model_name} (å·²æ ¡å‡†)\")\n",
    "\n",
    "# ---------------- 8.2 æ„å»º SHAP Explainer ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ„å»º SHAP Explainer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# èƒŒæ™¯æ•°æ®ï¼šä»è®­ç»ƒé›†ä¸­é‡‡æ ·\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "n_background = min(200, len(X_train_for_models))\n",
    "bg_idx = rng.choice(len(X_train_for_models), size=n_background, replace=False)\n",
    "X_background = X_train_for_models.iloc[bg_idx].copy()\n",
    "\n",
    "print(f\"èƒŒæ™¯æ ·æœ¬æ•°: {n_background} (æ¥è‡ªè®­ç»ƒé›†)\")\n",
    "\n",
    "# é¢„æµ‹å‡½æ•°\n",
    "def predict_proba_class1(X):\n",
    "    \"\"\"ç»Ÿä¸€çš„é¢„æµ‹å‡½æ•°ï¼šè¿”å›æ­£ç±»æ¦‚ç‡\"\"\"\n",
    "    if hasattr(shap_model, 'predict_proba'):\n",
    "        proba = shap_model.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "        return np.squeeze(proba)\n",
    "    elif hasattr(shap_model, 'decision_function'):\n",
    "        scores = shap_model.decision_function(X)\n",
    "        from scipy.special import expit\n",
    "        mu = np.mean(scores)\n",
    "        sigma = np.std(scores) + 1e-12\n",
    "        return expit((scores - mu) / sigma)\n",
    "    else:\n",
    "        pred = shap_model.predict(X)\n",
    "        return pred.astype(float)\n",
    "\n",
    "# åˆ›å»º Explainer\n",
    "try:\n",
    "    explainer = shap.Explainer(predict_proba_class1, X_background)\n",
    "    print(\"âœ… SHAP Explainer åˆ›å»ºæˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä½¿ç”¨å‡½æ•°å¼ Explainer å¤±è´¥: {e}\")\n",
    "    print(\"   å°è¯•ä½¿ç”¨ KernelExplainer...\")\n",
    "    explainer = shap.KernelExplainer(predict_proba_class1, X_background)\n",
    "    print(\"âœ… SHAP KernelExplainer åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "# è®¡ç®— SHAP å€¼ï¼ˆä»…æµ‹è¯•é›†ï¼‰\n",
    "print(f\"\\nè®¡ç®—æµ‹è¯•é›† SHAP å€¼ (n={len(X_shap)})...\")\n",
    "shap_values = explainer(X_shap)\n",
    "\n",
    "# æå–æ•°ç»„\n",
    "if hasattr(shap_values, 'values'):\n",
    "    shap_array = shap_values.values\n",
    "else:\n",
    "    shap_array = shap_values\n",
    "\n",
    "if shap_array.ndim == 3:\n",
    "    shap_array = shap_array[:, :, 1]\n",
    "\n",
    "print(f\"âœ… SHAP å€¼è®¡ç®—å®Œæˆ: {shap_array.shape}\")\n",
    "\n",
    "# ---------------- 8.3 å…¨å±€ç‰¹å¾é‡è¦æ€§ï¼šBeeswarm å›¾ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.3 å…¨å±€ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "plt.figure(figsize=(10, max(6, 0.35 * len(features_for_shap))))\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    features=X_shap,\n",
    "    feature_names=features_for_shap,\n",
    "    show=False,\n",
    "    max_display=len(features_for_shap)\n",
    ")\n",
    "plt.title(f\"SHAP Summary Plot â€” {best_model_name} (Critically Ill Patients)\", \n",
    "          fontsize=14, pad=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- 8.4 Dependence Plotsï¼ˆTop 9 ç‰¹å¾ï¼‰ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.4 SHAP Dependence Plots\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è®¡ç®—ç‰¹å¾é‡è¦æ€§å¹¶é€‰æ‹© Top 9\n",
    "mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "importance_df = pd.Series(mean_abs_shap, index=features_for_shap).sort_values(ascending=False)\n",
    "\n",
    "n_top = min(9, len(features_for_shap))\n",
    "top_features = importance_df.head(n_top).index.tolist()\n",
    "\n",
    "print(f\"ç»˜åˆ¶ Top {n_top} ç‰¹å¾çš„ä¾èµ–å›¾:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat} (Mean |SHAP|={importance_df[feat]:.4f})\")\n",
    "\n",
    "# å¸ƒå±€\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(n_top / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "axes = axes.flatten() if n_top > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # è·å–ç‰¹å¾ä½ç½®\n",
    "    feat_idx = features_for_shap.index(feature)\n",
    "    \n",
    "    # æ•°æ®\n",
    "    x_values = X_shap[feature].values\n",
    "    y_values = shap_array[:, feat_idx]\n",
    "    \n",
    "    # è¿‡æ»¤æ— æ•ˆå€¼\n",
    "    valid_mask = np.isfinite(x_values) & np.isfinite(y_values)\n",
    "    x_values = x_values[valid_mask]\n",
    "    y_values = y_values[valid_mask]\n",
    "    \n",
    "    # é¢œè‰²æ˜ å°„\n",
    "    if len(x_values) > 0:\n",
    "        vmin_feat = np.percentile(x_values, 1)\n",
    "        vmax_feat = np.percentile(x_values, 99)\n",
    "        norm_feat = mpl.colors.Normalize(vmin=vmin_feat, vmax=vmax_feat)\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            x_values, y_values,\n",
    "            c=x_values,\n",
    "            cmap='RdYlBu_r',\n",
    "            norm=norm_feat,\n",
    "            s=25,\n",
    "            alpha=0.6,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        # è‰²æ¡\n",
    "        cbar_ax = inset_axes(\n",
    "            ax,\n",
    "            width=\"3%\",\n",
    "            height=\"80%\",\n",
    "            loc='center right',\n",
    "            bbox_to_anchor=(0.12, 0, 1, 1),\n",
    "            bbox_transform=ax.transAxes,\n",
    "            borderpad=0\n",
    "        )\n",
    "        plt.colorbar(scatter, cax=cbar_ax)\n",
    "        \n",
    "        ax.set_xlabel(feature, fontsize=10)\n",
    "        ax.set_ylabel('SHAP value', fontsize=10)\n",
    "        ax.set_title(f'Dependence: {feature}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3, linestyle='--')\n",
    "        ax.axhline(0, color='black', linewidth=0.8, linestyle='-', alpha=0.3)\n",
    "\n",
    "# éšè—å¤šä½™å­å›¾\n",
    "for idx in range(n_top, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.suptitle(f'SHAP Dependence Plots â€” {best_model_name} (Critically Ill Patients)',\n",
    "             y=1.00, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- 8.5 ä¸ªä½“è§£é‡Šï¼šWaterfall å›¾ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.5 ä¸ªä½“è§£é‡Šï¼ˆWaterfall Plotsï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è®¡ç®—é¢„æµ‹æ¦‚ç‡\n",
    "y_pred_proba = predict_proba_class1(X_shap)\n",
    "\n",
    "# æ‰¾åˆ°æœ€é«˜å’Œæœ€ä½é£é™©æ ·æœ¬\n",
    "idx_high = np.argmax(y_pred_proba)\n",
    "idx_low = np.argmin(y_pred_proba)\n",
    "\n",
    "print(f\"æœ€é«˜æ­»äº¡é£é™©æ ·æœ¬: ç´¢å¼•={idx_high}, é¢„æµ‹æ¦‚ç‡={y_pred_proba[idx_high]:.4f}\")\n",
    "print(f\"æœ€ä½æ­»äº¡é£é™©æ ·æœ¬: ç´¢å¼•={idx_low}, é¢„æµ‹æ¦‚ç‡={y_pred_proba[idx_low]:.4f}\")\n",
    "\n",
    "def plot_waterfall_native(shap_values, X_data, sample_idx, title, max_display=15):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨SHAPåŸç”Ÿwaterfallå›¾\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # è·å– base value\n",
    "        if hasattr(shap_values, 'base_values'):\n",
    "            base_value = shap_values.base_values[sample_idx]\n",
    "        else:\n",
    "            base_value = explainer.expected_value\n",
    "            if isinstance(base_value, (list, np.ndarray)):\n",
    "                base_value = base_value[1] if len(base_value) > 1 else base_value[0]\n",
    "        \n",
    "        # æ„å»º Explanation å¯¹è±¡\n",
    "        explanation = shap.Explanation(\n",
    "            values=shap_array[sample_idx],\n",
    "            base_values=base_value,\n",
    "            data=X_data.iloc[sample_idx].values,\n",
    "            feature_names=features_for_shap\n",
    "        )\n",
    "        \n",
    "        # è®¾ç½®matplotlibå‚æ•°\n",
    "        plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "        plt.rcParams['font.size'] = 10\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        # åˆ›å»ºå›¾å½¢\n",
    "        fig = plt.figure(figsize=(12, max(10, 0.5 * max_display)))\n",
    "        \n",
    "        # ç»˜åˆ¶waterfallå›¾\n",
    "        shap.plots.waterfall(explanation, max_display=max_display, show=False)\n",
    "        \n",
    "        # è®¾ç½®æ ‡é¢˜\n",
    "        plt.gcf().suptitle(title, fontsize=14, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰“å°è¯¦ç»†æ•°æ®\n",
    "        print(f\"\\nè¯¥æ ·æœ¬çš„è¯¦ç»†ç‰¹å¾å€¼:\")\n",
    "        sample_data = X_data.iloc[sample_idx]\n",
    "        shap_vals = shap_array[sample_idx]\n",
    "        \n",
    "        # æŒ‰SHAPç»å¯¹å€¼æ’åº\n",
    "        abs_shap = np.abs(shap_vals)\n",
    "        sorted_idx = np.argsort(abs_shap)[::-1][:max_display]\n",
    "        \n",
    "        print(f\"{'ç‰¹å¾å':<25} {'ç‰¹å¾å€¼':>12} {'SHAPå€¼':>12}\")\n",
    "        print(\"-\" * 52)\n",
    "        for idx in sorted_idx:\n",
    "            feat_name = features_for_shap[idx]\n",
    "            feat_val = sample_data[feat_name]\n",
    "            shap_val = shap_vals[idx]\n",
    "            print(f\"{feat_name:<25} {feat_val:>12.4f} {shap_val:>12.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Waterfall å›¾ç»˜åˆ¶å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# é«˜é£é™©æ ·æœ¬\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"é«˜æ­»äº¡é£é™©æ ·æœ¬ Waterfall å›¾\")\n",
    "print(\"=\" * 70)\n",
    "plot_waterfall_native(\n",
    "    shap_values, X_shap, idx_high,\n",
    "    f\"High Mortality Risk Patient (Prob={y_pred_proba[idx_high]:.3f}) â€” {best_model_name}\",\n",
    "    max_display=15\n",
    ")\n",
    "\n",
    "# ä½é£é™©æ ·æœ¬\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ä½æ­»äº¡é£é™©æ ·æœ¬ Waterfall å›¾\")\n",
    "print(\"=\" * 70)\n",
    "plot_waterfall_native(\n",
    "    shap_values, X_shap, idx_low,\n",
    "    f\"Low Mortality Risk Patient (Prob={y_pred_proba[idx_low]:.3f}) â€” {best_model_name}\",\n",
    "    max_display=15\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ä¸ªä½“è§£é‡Šå®Œæˆ\")\n",
    "\n",
    "# ---------------- 8.6 ç‰¹å¾é‡è¦æ€§æ±‡æ€»è¡¨ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.6 ç‰¹å¾é‡è¦æ€§æ±‡æ€»\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Feature': features_for_shap,\n",
    "    'Mean_|SHAP|': np.abs(shap_array).mean(axis=0),\n",
    "    'Std_SHAP': shap_array.std(axis=0),\n",
    "    'Mean_SHAP': shap_array.mean(axis=0)\n",
    "})\n",
    "\n",
    "summary_df = summary_df.sort_values('Mean_|SHAP|', ascending=False).reset_index(drop=True)\n",
    "summary_df['Rank'] = range(1, len(summary_df) + 1)\n",
    "\n",
    "print(f\"\\nç‰¹å¾é‡è¦æ€§æ’åºï¼ˆåŸºäºæµ‹è¯•é›†ï¼Œn={len(X_shap)}ï¼‰:\")\n",
    "display(summary_df[['Rank', 'Feature', 'Mean_|SHAP|', 'Mean_SHAP', 'Std_SHAP']].round(4))\n",
    "\n",
    "# é‡ç‚¹å…³æ³¨ PTFQI\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â­ PTFQI åœ¨æ¨¡å‹ä¸­çš„è¡¨ç°\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ptfqi_info = summary_df[summary_df['Feature'] == 'PTFQI']\n",
    "if not ptfqi_info.empty:\n",
    "    ptfqi_rank = int(ptfqi_info['Rank'].values[0])\n",
    "    ptfqi_shap = float(ptfqi_info['Mean_|SHAP|'].values[0])\n",
    "    ptfqi_mean_shap = float(ptfqi_info['Mean_SHAP'].values[0])\n",
    "    \n",
    "    print(f\"\\nPTFQI è¯¦ç»†ä¿¡æ¯:\")\n",
    "    print(f\"  é‡è¦æ€§æ’å: {ptfqi_rank}/{len(summary_df)}\")\n",
    "    print(f\"  Mean |SHAP|: {ptfqi_shap:.4f}\")\n",
    "    print(f\"  Mean SHAP (æ–¹å‘æ€§): {ptfqi_mean_shap:+.4f}\")\n",
    "    \n",
    "    # ä¸Topç‰¹å¾å¯¹æ¯”\n",
    "    top1_shap = float(summary_df.iloc[0]['Mean_|SHAP|'])\n",
    "    top1_name = summary_df.iloc[0]['Feature']\n",
    "    print(f\"\\n  ä¸ Top 1 ({top1_name}) çš„å¯¹æ¯”:\")\n",
    "    print(f\"    Top 1 Mean |SHAP|: {top1_shap:.4f}\")\n",
    "    print(f\"    PTFQI / Top 1 æ¯”å€¼: {ptfqi_shap/top1_shap:.2%}\")\n",
    "    \n",
    "    # è§£é‡Šæ–¹å‘æ€§\n",
    "    if ptfqi_mean_shap > 0:\n",
    "        print(f\"\\n  âš ï¸ PTFQI å¹³å‡æ•ˆåº”ä¸ºæ­£ ({ptfqi_mean_shap:+.4f})\")\n",
    "        print(f\"      â†’ PTFQI å‡é«˜æ€»ä½“ä¸Šä¸æ­»äº¡é£é™©**å¢åŠ **ç›¸å…³\")\n",
    "    elif ptfqi_mean_shap < 0:\n",
    "        print(f\"\\n  âœ“ PTFQI å¹³å‡æ•ˆåº”ä¸ºè´Ÿ ({ptfqi_mean_shap:+.4f})\")\n",
    "        print(f\"      â†’ PTFQI å‡é«˜æ€»ä½“ä¸Šä¸æ­»äº¡é£é™©**é™ä½**ç›¸å…³\")\n",
    "    else:\n",
    "        print(f\"\\n  - PTFQI å¹³å‡æ•ˆåº”æ¥è¿‘0 ({ptfqi_mean_shap:+.4f})\")\n",
    "        print(f\"      â†’ PTFQI å¯¹æ­»äº¡é£é™©çš„å½±å“åœ¨äººç¾¤ä¸­è¾ƒä¸ºå¹³è¡¡\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ æœªåœ¨ç‰¹å¾åˆ—è¡¨ä¸­æ‰¾åˆ° PTFQI\")\n",
    "\n",
    "# ---------------- 8.7 PTFQI æ·±å…¥åˆ†æ ----------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8.7 PTFQI æ·±å…¥åˆ†æï¼ˆæ ¸å¿ƒç ”ç©¶å˜é‡ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'PTFQI' in features_for_shap:\n",
    "    ptfqi_idx = features_for_shap.index('PTFQI')\n",
    "    ptfqi_values = X_shap['PTFQI'].values\n",
    "    ptfqi_shap_values = shap_array[:, ptfqi_idx]\n",
    "    \n",
    "    # 1. PTFQI åŸºæœ¬ç»Ÿè®¡\n",
    "    print(f\"\\n1. PTFQI ç‰¹å¾å€¼åˆ†å¸ƒ:\")\n",
    "    print(f\"   å‡å€¼ Â± æ ‡å‡†å·®: {np.mean(ptfqi_values):.3f} Â± {np.std(ptfqi_values):.3f}\")\n",
    "    print(f\"   ä¸­ä½æ•° [IQR]: {np.median(ptfqi_values):.3f} [{np.percentile(ptfqi_values, 25):.3f}, {np.percentile(ptfqi_values, 75):.3f}]\")\n",
    "    print(f\"   èŒƒå›´: [{np.min(ptfqi_values):.3f}, {np.max(ptfqi_values):.3f}]\")\n",
    "    \n",
    "    print(f\"\\n2. PTFQI SHAP å€¼åˆ†å¸ƒ:\")\n",
    "    print(f\"   å‡å€¼: {np.mean(ptfqi_shap_values):.4f}\")\n",
    "    print(f\"   æ ‡å‡†å·®: {np.std(ptfqi_shap_values):.4f}\")\n",
    "    print(f\"   èŒƒå›´: [{np.min(ptfqi_shap_values):.4f}, {np.max(ptfqi_shap_values):.4f}]\")\n",
    "    \n",
    "    # 2. å•å˜é‡åˆ†æ\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    \n",
    "    death_group = y_shap.values == 1\n",
    "    ptfqi_death = ptfqi_values[death_group]\n",
    "    ptfqi_alive = ptfqi_values[~death_group]\n",
    "    \n",
    "    print(f\"\\n3. å•å˜é‡åˆ†æï¼ˆPTFQI vs å®é™…æ­»äº¡ï¼‰:\")\n",
    "    print(f\"   æ­»äº¡ç»„: {np.mean(ptfqi_death):.3f} Â± {np.std(ptfqi_death):.3f} (n={len(ptfqi_death)})\")\n",
    "    print(f\"   å­˜æ´»ç»„: {np.mean(ptfqi_alive):.3f} Â± {np.std(ptfqi_alive):.3f} (n={len(ptfqi_alive)})\")\n",
    "    \n",
    "    u_stat, p_value = mannwhitneyu(ptfqi_death, ptfqi_alive, alternative='two-sided')\n",
    "    print(f\"   Mann-Whitney U æ£€éªŒ: U={u_stat:.1f}, p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"   âœ… å·®å¼‚æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ (p<0.05)\")\n",
    "        if np.mean(ptfqi_death) > np.mean(ptfqi_alive):\n",
    "            print(f\"      æ­»äº¡ç»„ PTFQI æ˜¾è‘—æ›´é«˜\")\n",
    "        else:\n",
    "            print(f\"      æ­»äº¡ç»„ PTFQI æ˜¾è‘—æ›´ä½\")\n",
    "    else:\n",
    "        print(f\"   - å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰ (pâ‰¥0.05)\")\n",
    "    \n",
    "    # 3. ä¸é¢„æµ‹æ¦‚ç‡çš„ç›¸å…³æ€§\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    pred_proba = predict_proba_class1(X_shap)\n",
    "    corr, pval = spearmanr(ptfqi_values, pred_proba)\n",
    "    \n",
    "    print(f\"\\n4. PTFQI ä¸æ¨¡å‹é¢„æµ‹æ¦‚ç‡çš„ç›¸å…³æ€§:\")\n",
    "    print(f\"   Spearman r = {corr:.4f}, p = {pval:.4e}\")\n",
    "    \n",
    "    if pval < 0.05:\n",
    "        print(f\"   âœ… ç›¸å…³æ€§æœ‰ç»Ÿè®¡å­¦æ„ä¹‰\")\n",
    "        if corr > 0:\n",
    "            print(f\"      PTFQI å‡é«˜ â†’ é¢„æµ‹æ­»äº¡æ¦‚ç‡å‡é«˜\")\n",
    "        else:\n",
    "            print(f\"      PTFQI å‡é«˜ â†’ é¢„æµ‹æ­»äº¡æ¦‚ç‡é™ä½\")\n",
    "    \n",
    "    # 4. å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # å·¦å›¾ï¼šPTFQI vs SHAPå€¼\n",
    "    scatter1 = axes[0].scatter(ptfqi_values, ptfqi_shap_values, \n",
    "                               c=ptfqi_values, cmap='RdYlBu_r', s=40, alpha=0.6)\n",
    "    axes[0].axhline(0, color='black', linewidth=0.8, linestyle='-', alpha=0.3)\n",
    "    axes[0].set_xlabel('PTFQI Value', fontsize=12)\n",
    "    axes[0].set_ylabel('SHAP Value for PTFQI', fontsize=12)\n",
    "    axes[0].set_title('PTFQI Dependence Plot', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='PTFQI Value')\n",
    "    \n",
    "    # ä¸­å›¾ï¼šPTFQI vs å®é™…æ­»äº¡\n",
    "    bp = axes[1].boxplot([ptfqi_alive, ptfqi_death], \n",
    "                          labels=['Survived', 'Died'],\n",
    "                          patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], ['lightblue', 'lightcoral']):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[1].set_ylabel('PTFQI Value', fontsize=12)\n",
    "    axes[1].set_xlabel('Actual Outcome', fontsize=12)\n",
    "    axes[1].set_title(f'PTFQI by Outcome\\n(Mann-Whitney p={p_value:.4f})', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # å³å›¾ï¼šPTFQI vs é¢„æµ‹æ¦‚ç‡\n",
    "    scatter2 = axes[2].scatter(ptfqi_values, pred_proba, \n",
    "                               c=y_shap.values, cmap='RdYlBu_r', s=40, alpha=0.6)\n",
    "    axes[2].set_xlabel('PTFQI Value', fontsize=12)\n",
    "    axes[2].set_ylabel('Predicted Mortality Probability', fontsize=12)\n",
    "    axes[2].set_title(f'PTFQI vs Prediction\\n(Spearman r={corr:.3f}, p={pval:.1e})', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    cbar = plt.colorbar(scatter2, ax=axes[2])\n",
    "    cbar.set_label('Actual Outcome', fontsize=10)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels(['Survived', 'Died'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. æŒ‰ PTFQI åˆ†ç»„åˆ†æ\n",
    "    print(f\"\\n5. æŒ‰ PTFQI ä¸‰åˆ†ä½åˆ†ç»„åˆ†æ:\")\n",
    "    try:\n",
    "        ptfqi_terciles = pd.qcut(ptfqi_values, q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
    "        \n",
    "        for group in ['Low', 'Medium', 'High']:\n",
    "            if group in ptfqi_terciles.values:\n",
    "                group_mask = ptfqi_terciles == group\n",
    "                group_actual = y_shap.values[group_mask]\n",
    "                group_pred = pred_proba[group_mask]\n",
    "                \n",
    "                print(f\"\\n   {group} PTFQI ç»„:\")\n",
    "                print(f\"     æ ·æœ¬æ•°: {np.sum(group_mask)}\")\n",
    "                print(f\"     PTFQI èŒƒå›´: [{np.min(ptfqi_values[group_mask]):.3f}, {np.max(ptfqi_values[group_mask]):.3f}]\")\n",
    "                print(f\"     å®é™…æ­»äº¡ç‡: {np.mean(group_actual):.2%}\")\n",
    "                print(f\"     å¹³å‡é¢„æµ‹æ¦‚ç‡: {np.mean(group_pred):.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ åˆ†ç»„åˆ†æå¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ç¬¬8æ­¥ï¼šSHAP è§£é‡Šåˆ†æå®Œæˆ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 8.3c Fig.5 â€” SHAP dependence plots ----------------\n",
    "# 1) å–æ­£ç±» SHAP æ•°ç»„ï¼ˆå…¼å®¹å¤šåˆ†ç±»/æ—§ç‰ˆshapï¼‰\n",
    "sv_arr = sv.values if hasattr(sv, \"values\") else sv\n",
    "sv_arr = np.asarray(sv_arr)\n",
    "if sv_arr.ndim == 3:   # [n_sample, n_feat, n_class]\n",
    "    sv_arr = sv_arr[..., 1]\n",
    "\n",
    "# --- é˜²å¾¡ï¼šsv ä¸ X_explain è¡Œæ•°ã€åˆ—æ•°å¿…é¡»åŒ¹é… ---\n",
    "assert sv_arr.shape[0] == len(X_explain), \\\n",
    "    f\"sv è¡Œæ•°({sv_arr.shape[0]})ä¸ X_explain({len(X_explain)})ä¸ä¸€è‡´\"\n",
    "assert sv_arr.shape[1] == X_explain.shape[1], \\\n",
    "    f\"sv åˆ—æ•°({sv_arr.shape[1]})ä¸ X_explain({X_explain.shape[1]})ä¸ä¸€è‡´ï¼›è¯·ç¡®è®¤ explainer è¾“å…¥åˆ—å³ä¸º X_explain.columns\"\n",
    "\n",
    "# 2) åªåœ¨â€œå…¥é€‰ç‰¹å¾â€é‡ŒæŒ‘å‰ Kï¼ˆä¸è¶³ K åˆ™å…¨ç”¨ï¼‰\n",
    "base_cols = list(X_explain.columns)  # ä¸ explainer è¾“å…¥ä¿æŒä¸€è‡´\n",
    "topK = 9\n",
    "topK = min(topK, len(base_cols))\n",
    "imp_series = pd.Series(np.nanmean(np.abs(sv_arr), axis=0), index=base_cols)\n",
    "feats_dep = imp_series.sort_values(ascending=False).head(topK).index.tolist()\n",
    "\n",
    "# 3) è°ƒè‰²ï¼šçº¢â†’é»„â†’è“ï¼ˆç”¨ _rï¼‰\n",
    "cmap = mpl.cm.get_cmap(\"RdYlBu_r\")\n",
    "\n",
    "# 4) å­å›¾ç½‘æ ¼\n",
    "n = len(feats_dep)\n",
    "if n == 0:\n",
    "    print(\"âš ï¸ ä¾èµ–å›¾è·³è¿‡ï¼šæ²¡æœ‰å¯ç”¨çš„ç‰¹å¾ï¼ˆtopK=0ï¼‰ã€‚\")\n",
    "else:\n",
    "    # è‡ªé€‚åº”åˆ—æ•°ï¼šç‰¹å¾å¤šæ—¶ 3 åˆ—ï¼Œå¦åˆ™ 2 åˆ—\n",
    "    ncol = 3 if n >= 6 else 2\n",
    "    nrow = math.ceil(n / ncol)\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(7.2*ncol, 4.8*nrow))\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for i, f in enumerate(feats_dep):\n",
    "        r, c = divmod(i, ncol)\n",
    "        ax = axes[r, c]\n",
    "\n",
    "        # x: åŸå§‹å–å€¼ï¼›y: å¯¹åº” SHAPï¼ˆç”¨ X_explain çš„åˆ—ç´¢å¼•ç¡®ä¿å¯¹é½ï¼‰\n",
    "        col_idx = X_explain.columns.get_loc(f)\n",
    "        x = X_explain[f].to_numpy()\n",
    "        y = sv_arr[:, col_idx]\n",
    "\n",
    "        # --- æ¸…ç†ï¼šå»é™¤ NaN/inf\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        if not np.any(mask):\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        x = x[mask]; y = y[mask]\n",
    "\n",
    "        # --- å¤„ç†å¼‚å¸¸å€¼ï¼šæŒ‰åˆ†ä½è£å‰ªï¼Œé¿å…æç«¯ç‚¹æŠŠåæ ‡è½´æ‹‰çˆ† ---\n",
    "        if len(y) > 10:\n",
    "            y_lo, y_hi = np.nanpercentile(y, [0.5, 99.5])\n",
    "            y = np.clip(y, y_lo, y_hi)\n",
    "\n",
    "        # 5) æ¯ä¸ªå­å›¾ç‹¬ç«‹è‰²æ ‡ï¼ˆé¿å…é‡çº²å·®å¼‚ï¼‰\n",
    "        vmin = float(np.nanmin(x)); vmax = float(np.nanmax(x))\n",
    "        # å¸¸æ•°åˆ—ä¿æŠ¤ï¼švmin==vmax ä¼šå¯¼è‡´ Normalize æŠ¥é”™\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax):\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        if vmin == vmax:\n",
    "            vmin -= 1e-9; vmax += 1e-9\n",
    "        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        sc = ax.scatter(\n",
    "            x, y, c=x, cmap=cmap, norm=norm,\n",
    "            s=22, alpha=0.75, linewidths=0\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Pneumonia Feature Dependence â€” {f}\", fontsize=12.5)\n",
    "        ax.set_xlabel(f, fontsize=10.5)\n",
    "        ax.set_ylabel(\"SHAP value\", fontsize=10.5)\n",
    "        ax.grid(alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "        # 6) å³ä¾§ç‹¬ç«‹è‰²æ¡ï¼ˆæ›´çª„ã€æ›´é å¤–ï¼‰\n",
    "        cax = inset_axes(ax, width=\"3.0%\", height=\"78%\", loc='center right',\n",
    "                         bbox_to_anchor=(0.01, 0, 1, 1),\n",
    "                         bbox_transform=ax.transAxes, borderpad=0)\n",
    "        cb = plt.colorbar(sc, cax=cax)\n",
    "        cb.ax.tick_params(labelsize=8.5)\n",
    "\n",
    "    # 7) å…³é—­å¤šä½™å­å›¾\n",
    "    for j in range(i+1, nrow*ncol):\n",
    "        r, c = divmod(j, ncol)\n",
    "        axes[r, c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(pad=2.0, h_pad=2.6, w_pad=2.6)\n",
    "    fig.suptitle(\n",
    "        f\"SHAP dependence plots â€” {best_model_name}{' (Calibrated)' if 'calibrated_best' in globals() else ''} @ {which_split}\",\n",
    "        y=1.02, fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Force-like å¤šæ ·æœ¬æŠ˜çº¿å›¾ï¼ˆç¨³å¥ç‰ˆï¼Œé€‚é…æ ¡å‡†åæ¨¡å‹ï¼‰ ========\n",
    "def _proba_from_any(est, X_df):\n",
    "    \"\"\"\n",
    "    ä»ä»»æ„äºŒåˆ†ç±»ä¼°è®¡å™¨æ‹¿â€œæ­£ç±»æ¦‚ç‡â€æ ·å¼åˆ†æ•°ã€‚\n",
    "    å…¼å®¹ CalibratedClassifierCV / Pipeline / SVM(decision_function) / åªæœ‰ predict çš„æ¨¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    if hasattr(est, \"predict_proba\"):\n",
    "        p = est.predict_proba(X_df)\n",
    "        return p[:, 1] if p.ndim == 2 and p.shape[1] >= 2 else np.squeeze(p).astype(float)\n",
    "    if hasattr(est, \"decision_function\"):\n",
    "        s = np.asarray(est.decision_function(X_df), dtype=float)\n",
    "        # z-score åè¿‡ sigmoidï¼Œé¿å…å°ºåº¦å·®å¼‚\n",
    "        mu, sd = float(np.mean(s)), float(np.std(s) + 1e-12)\n",
    "        from scipy.special import expit\n",
    "        return expit((s - mu) / sd)\n",
    "    pred = np.squeeze(est.predict(X_df)).astype(float)\n",
    "    if set(np.unique(pred)).issubset({0.0, 1.0}):\n",
    "        eps = 1e-6\n",
    "        return pred * (1 - 2 * eps) + eps\n",
    "    # æœ€åå…œåº•ï¼šmin-max åˆ°(0,1)\n",
    "    pmin, pmax = np.min(pred), np.max(pred)\n",
    "    return (pred - pmin) / (pmax - pmin + 1e-12)\n",
    "\n",
    "\n",
    "def plot_force_like_lines(\n",
    "    sv, X_df, title=\"\",\n",
    "    top_n=10,\n",
    "    smooth=0,                # æ»‘åŠ¨çª—å£å¹³æ»‘ï¼ˆ>1 å¼€å¯ï¼‰\n",
    "    legend_out=True,\n",
    "    order_scores=None,       # è‹¥æä¾›åˆ™æŒ‰æ¦‚ç‡ç”±é«˜åˆ°ä½æ’åº\n",
    "    fixed_top_idx=None,      # è‹¥æä¾›åˆ™å¼ºåˆ¶ä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„åˆ—ç´¢å¼•\n",
    "    ylim=None,\n",
    "    max_legend=10,           # å›¾ä¾‹æœ€å¤šæ˜¾ç¤ºå¤šå°‘æ¡\n",
    "    clip_pct=(0.5, 99.5)     # y æŒ‰åˆ†ä½è£å‰ªï¼ŒæŠ‘åˆ¶æç«¯ç‚¹\n",
    "):\n",
    "    \"\"\"\n",
    "    ç”¨æŠ˜çº¿æ–¹å¼å¤ç° Fig.6 é£æ ¼çš„â€œå¤šæ ·æœ¬ force plotâ€ï¼ˆé™æ€ PNGï¼‰ã€‚\n",
    "    é‡è¦ï¼šsv å¿…é¡»ä¸ X_df çš„è¡Œåˆ—å®Œå…¨ä¸€è‡´ï¼ˆåŒæ ·æœ¬ã€åŒåˆ—é¡ºåºï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    # ----- 1) å– SHAP æ•°ç»„ï¼ˆæ­£ç±»ï¼‰ -----\n",
    "    sv_values = sv.values if hasattr(sv, \"values\") else sv\n",
    "    sv_values = np.asarray(sv_values)\n",
    "    if sv_values.ndim == 3:\n",
    "        sv_values = sv_values[..., 1]\n",
    "\n",
    "    # å®‰å…¨æ£€æŸ¥\n",
    "    n_samples, n_feats = sv_values.shape\n",
    "    feat_cols = np.array(X_df.columns.tolist())\n",
    "    assert n_feats == len(feat_cols), \\\n",
    "        f\"sv åˆ—æ•°({n_feats})ä¸ X_df åˆ—æ•°({len(feat_cols)})ä¸ä¸€è‡´ï¼›è¯·ä¿è¯ä¸ explainer è¾“å…¥ä¸€è‡´ã€‚\"\n",
    "    assert n_samples == len(X_df), \\\n",
    "        f\"sv è¡Œæ•°({n_samples})ä¸ X_df({len(X_df)})ä¸ä¸€è‡´ï¼›ä¸è¦åœ¨è§£é‡Šåå†é‡æ’/æŠ½æ ·ã€‚\"\n",
    "\n",
    "    # ----- 2) æ ·æœ¬æ’åº -----\n",
    "    if order_scores is not None:\n",
    "        order_scores = np.asarray(order_scores)\n",
    "        assert order_scores.shape[0] == n_samples, \"order_scores ä¸æ ·æœ¬æ•°ä¸ä¸€è‡´ã€‚\"\n",
    "        order = np.argsort(order_scores)[::-1]  # æ¦‚ç‡ä»é«˜åˆ°ä½\n",
    "    else:\n",
    "        order = np.argsort(np.sum(np.abs(sv_values), axis=1))[::-1]  # |SHAP| æ€»å’Œ\n",
    "    S = sv_values[order, :]\n",
    "\n",
    "    # ----- 3) é€‰å‰ top_n ç‰¹å¾ï¼ˆæˆ–ä½¿ç”¨å›ºå®šç´¢å¼•ï¼‰ -----\n",
    "    top_n = int(max(1, min(top_n, n_feats)))\n",
    "    if fixed_top_idx is None:\n",
    "        mean_abs = np.nanmean(np.abs(S), axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[-top_n:][::-1]\n",
    "    else:\n",
    "        top_idx = np.array(fixed_top_idx, dtype=int)\n",
    "        top_idx = top_idx[(top_idx >= 0) & (top_idx < n_feats)]\n",
    "        if len(top_idx) == 0:\n",
    "            raise ValueError(\"fixed_top_idx å…¨éƒ¨è¶Šç•Œã€‚\")\n",
    "    feat_names = feat_cols[top_idx].tolist()\n",
    "\n",
    "    # ----- 4) é…è‰²ï¼šé«˜é¥±å’Œå¯¹æ¯” -----\n",
    "    palette = [\n",
    "        \"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\",\n",
    "        \"#ffff33\", \"#a65628\", \"#f781bf\", \"#999999\", \"#66c2a5\",\n",
    "        \"#1b9e77\", \"#d95f02\", \"#7570b3\"  # ä»¥é˜² top_n > 10\n",
    "    ]\n",
    "    colors = [palette[i % len(palette)] for i in range(len(top_idx))]\n",
    "\n",
    "    # ----- 5) ç»˜å›¾ -----\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(S.shape[0])\n",
    "\n",
    "    # y è½´åˆ†ä½è£å‰ªèŒƒå›´é¢„å…ˆç®—å¥½ï¼ˆä¾¿äºä¸¤æ¬¡è°ƒç”¨ä¿æŒä¸€è‡´ï¼‰\n",
    "    if ylim is None and clip_pct is not None:\n",
    "        y_all = S[:, top_idx].ravel()\n",
    "        y_all = y_all[np.isfinite(y_all)]\n",
    "        if y_all.size > 20:\n",
    "            lo, hi = np.nanpercentile(y_all, list(clip_pct))\n",
    "            ylim = (float(lo), float(hi))\n",
    "\n",
    "    for k, (c, j, name) in enumerate(zip(colors, top_idx, feat_names)):\n",
    "        y = S[:, j].astype(float)\n",
    "        if ylim is not None:\n",
    "            y = np.clip(y, ylim[0], ylim[1])\n",
    "        if smooth and smooth > 1:\n",
    "            y = pd.Series(y).rolling(int(smooth), min_periods=1, center=True).mean().values\n",
    "        lw = 1.8 if k < max_legend else 1.2  # å¤šä½™çº¿æ¡ç•¥ç»†\n",
    "        plt.plot(x, y, color=c, lw=lw, alpha=0.95, label=name if k < max_legend else None)\n",
    "\n",
    "    plt.axhline(0, color=\"black\", lw=1.0)\n",
    "    plt.xlim(0, S.shape[0])\n",
    "    if ylim is not None:\n",
    "        # å†ç¨å¾®ç•™ç™½ï¼Œè§†è§‰ä¸è´´è¾¹\n",
    "        pad = 0.03 * (ylim[1] - ylim[0])\n",
    "        plt.ylim(ylim[0] - pad, ylim[1] + pad)\n",
    "\n",
    "    plt.xlabel(\"Pneumonia Patients\", fontsize=11)\n",
    "    plt.ylabel(\"SHAP value contribution\", fontsize=11)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=13, pad=10, fontweight=\"bold\")\n",
    "\n",
    "    # ----- 6) å›¾ä¾‹ï¼šæ§åˆ¶æ•°é‡ï¼Œé¿å…æº¢å‡º -----\n",
    "    if legend_out:\n",
    "        plt.legend(\n",
    "            title=\"Top features\",\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.03, 0.5),\n",
    "            frameon=False,\n",
    "            fontsize=9.5,\n",
    "            title_fontsize=10.5\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "    else:\n",
    "        plt.legend(frameon=False, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    return top_idx\n",
    "\n",
    "\n",
    "# â€”â€” é€‰æ‹©ç”¨äºç”Ÿæˆæ¦‚ç‡çš„â€œæœ€ç»ˆæ¨¡å‹â€ï¼šä¼˜å…ˆæ ¡å‡†åçš„ â€”â€” \n",
    "_final_est_for_plot = calibrated_best if ('calibrated_best' in globals()) else best_est\n",
    "\n",
    "# â€”â€” è®­ç»ƒé›†ï¼šæŒ‰æ¨¡å‹â€œæ¦‚ç‡â€æ’åºï¼Œè¿”å› top_idx â€”â€” \n",
    "train_scores = _proba_from_any(_final_est_for_plot, train_X_imp.loc[:, selected_all])\n",
    "top_idx = plot_force_like_lines(\n",
    "    sv_train,\n",
    "    train_X_imp.loc[:, selected_all],\n",
    "    title=f\"Force-like plot â€” MIMIC-IV (Train){' (Calibrated)' if 'calibrated_best' in globals() else ''}\",\n",
    "    top_n=10,\n",
    "    smooth=5,\n",
    "    order_scores=train_scores\n",
    ")\n",
    "\n",
    "# â€”â€” æµ‹è¯•é›†ï¼šç”¨è®­ç»ƒé˜¶æ®µçš„ top_idxï¼›ä¿æŒ y è½´ä¸€è‡´ä¾¿äºå¯¹æ¯” â€”â€” \n",
    "test_scores = _proba_from_any(_final_est_for_plot, test_X_imp.loc[:, selected_all])\n",
    "\n",
    "# ç»Ÿä¸€ y è½´èŒƒå›´ï¼šåŸºäº train/test ä¸¤è¾¹çš„ sv å…±åŒå†³å®šï¼ˆç¨³ï¼‰\n",
    "_sv_train_arr = sv_train.values if hasattr(sv_train, \"values\") else sv_train\n",
    "if _sv_train_arr.ndim == 3: _sv_train_arr = _sv_train_arr[..., 1]\n",
    "_sv_test_arr  = sv_test.values  if hasattr(sv_test, \"values\")  else sv_test\n",
    "if _sv_test_arr.ndim  == 3: _sv_test_arr  = _sv_test_arr[..., 1]\n",
    "\n",
    "y_abs_max = np.nanmax(np.abs(np.concatenate([_sv_train_arr[:, top_idx], _sv_test_arr[:, top_idx]], axis=0)))\n",
    "# ç»™ä¸€ç‚¹ä½™é‡\n",
    "ylim_shared = (-1.05 * y_abs_max, 1.05 * y_abs_max)\n",
    "\n",
    "_ = plot_force_like_lines(\n",
    "    sv_test,\n",
    "    test_X_imp.loc[:, selected_all],\n",
    "    title=f\"Force-like plot â€” MIMIC-IV (Test){' (Calibrated)' if 'calibrated_best' in globals() else ''}\",\n",
    "    smooth=5,\n",
    "    order_scores=test_scores,\n",
    "    fixed_top_idx=top_idx,\n",
    "    ylim=ylim_shared\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
